{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Caution: pyvista plots require a machine with attached physical display or a display emulation (Xvfb package) - otherwise the kernel may die when generating renderings.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyvista\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpv\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnibabel\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnib\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mCaution: pyvista plots require a machine with attached physical display or a display emulation (Xvfb package) - otherwise the kernel may die when generating renderings.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Caution: pyvista plots require a machine with attached physical display or a display emulation (Xvfb package) - otherwise the kernel may die when generating renderings."
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import pymeshfix as mf\n",
    "import nibabel as nib\n",
    "import pyvista as pv\n",
    "import nibabel as nib\n",
    "\n",
    "assert False, \"Caution: pyvista plots require a machine with attached physical display or a display emulation (Xvfb package) - otherwise the kernel may die when generating renderings.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create mesh from shape voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import h5py\n",
    "import pymeshfix as mf\n",
    "import nibabel as nib\n",
    "import pyvista as pv\n",
    "from pathlib import Path\n",
    "from skimage import measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_label_values(label):\n",
    "    # Replace label numbers with MMWHS equivalent\n",
    "    # STRUCTURE           MMWHS   ACDC    NNUNET\n",
    "    # background          0       0       0\n",
    "    # left_myocardium     205     2       1\n",
    "    # left_atrium         420     N/A     2\n",
    "    # ?                   421     N/A     N/A\n",
    "    # left_ventricle      500     3       3\n",
    "    # right_atrium        550     N/A     4\n",
    "    # right_ventricle     600     1       5\n",
    "    # ascending_aorta     820     N/A     6\n",
    "    # pulmonary_artery    850     N/A     7\n",
    "    orig_values = [0,  205, 420, 421, 500, 550, 600, 820, 850]\n",
    "    new_values = [0,  1,   2,   0,   3,   4,   5,   0,   0]\n",
    "\n",
    "    modified_label = label.clone()\n",
    "    for orig, new in zip(orig_values, new_values):\n",
    "        modified_label[modified_label == orig] = new\n",
    "    return modified_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_shape = nib.load(\"mr_train_1004_label_registered.nii.gz\")\n",
    "shape_data = replace_label_values(torch.as_tensor(nii_shape.get_fdata())).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_affine = torch.as_tensor(nii_shape.affine)\n",
    "image_sample = nib.load(\"mr_train_1004_image_registered.nii.gz\").get_fdata()\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "CLASSES = ['background', 'MYO', 'LA', 'LV', 'RA', 'RV']\n",
    "\n",
    "heart_data = {}\n",
    "for class_idx, tag in enumerate(CLASSES):\n",
    "    if class_idx == 0: continue\n",
    "\n",
    "    sub_label = torch.nn.functional.one_hot(shape_data.long(), len(CLASSES))[:,:,:, class_idx]\n",
    "    verts, faces, normals, values = measure.marching_cubes(sub_label.cpu().numpy(), spacing=SPACING, step_size=STEP_SIZE)\n",
    "    data = dict(\n",
    "        verts=torch.as_tensor(verts.copy()),\n",
    "        faces=torch.as_tensor(faces.copy()),\n",
    "        normals=torch.as_tensor(normals.copy()), \n",
    "        values=torch.as_tensor(values.copy())\n",
    "    )\n",
    "    heart_data[tag] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_mat = torch.from_numpy(np.loadtxt(\"mmwhs_1002_HLA_red_slice_to_ras.mat\"))\n",
    "sa_mat = torch.from_numpy(np.loadtxt(\"mmwhs_1002_SA_yellow_slice_to_ras.mat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_sa_mat_1004 = torch.tensor(\n",
    "    [[[ 0.3762,  0.0235, -0.9262,  0.0000],\n",
    "      [ 0.6500,  0.7056,  0.2820,  0.0000],\n",
    "      [ 0.6602, -0.7082,  0.2502,  0.0000],\n",
    "      [ 0.0000,  0.0000,  0.0000,  1.0000]]]\n",
    ")\n",
    "\n",
    "optimized_hla_mat_1004 = torch.tensor(\n",
    "    [[[ 0.8096, -0.5865,  0.0236,  0.0000],\n",
    "      [ 0.5751,  0.7844, -0.2325,  0.0000],\n",
    "      [ 0.1178,  0.2018,  0.9723,  0.0000],\n",
    "      [ 0.0000,  0.0000,  0.0000,  1.0000]]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christianweihsbach/code/shapeformer_postprocessing/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    }
   ],
   "source": [
    "from align_mmwhs import nifti_transform\n",
    "FOV_MM = torch.tensor([224,224,224])\n",
    "FOV_VOX = torch.tensor([160,160,160])\n",
    "\n",
    "with torch.no_grad():\n",
    "    hla_label, hla_affine = nifti_transform(shape_data.unsqueeze(0).unsqueeze(0), shape_affine.unsqueeze(0), hla_mat.unsqueeze(0), fov_mm=FOV_MM, fov_vox=FOV_VOX, is_label=True, pre_grid_sample_affine=None)\n",
    "    sa_label, sa_affine = nifti_transform(shape_data.unsqueeze(0).unsqueeze(0), shape_affine.unsqueeze(0), sa_mat.unsqueeze(0), fov_mm=FOV_MM, fov_vox=FOV_VOX, is_label=True, pre_grid_sample_affine=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0390, 0.5021, 0.5065], dtype=torch.float64) tensor([64., 64., 64.])\n",
      "tensor([ 0.2684, -0.1490,  0.6450], dtype=torch.float64) tensor([64., 64., 64.])\n"
     ]
    }
   ],
   "source": [
    "sa_normal = (sa_affine.inverse() @ torch.tensor([0.,0.,1.,0.]).double())[0,:3]\n",
    "sa_support = torch.tensor([64.,64.,64.])\n",
    "# sa_support = (sa_affine.inverse() @ torch.tensor([0.,0.,0.,1.]).to(dtype=sa_to_hla.dtype))[0,:3]\n",
    "print(sa_normal, sa_support)\n",
    "\n",
    "hla_normal = (hla_affine.inverse() @ torch.tensor([0.,0.,1.,0.]).double())[0,:3]\n",
    "hla_support = torch.tensor([64.,64.,64.])\n",
    "print(hla_normal, hla_support)\n",
    "# hla_support = (hla_to_sa @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_vertices = heart_data['MYO']['verts']\n",
    "h_faces = heart_data['MYO']['faces']\n",
    "\n",
    "for tag, data in heart_data.items():\n",
    "    faces = data['faces']\n",
    "    num_faces = faces.shape[0]\n",
    "    num_points = torch.tensor([3]*num_faces).view(num_faces,1)\n",
    "    data['pyvista_faces'] = torch.cat([num_points, faces], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://coolors.co/b8336a-726da8-7d8cc4-a0d2db-c490d1\n",
    "\n",
    "palette = [\n",
    "    '#B8336A',\n",
    "    '#726DA8',\n",
    "    '#7D8CC4',\n",
    "    '#A0D2DB',\n",
    "    '#C490D1',\n",
    "]\n",
    "dark_palette = [\n",
    "    '#4F172E', \n",
    "    '#424064',\n",
    "    '#485070',\n",
    "    '#547378',\n",
    "    '#73507C',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cd1c8a9bc14691b82925b607117b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=2000, layout=Layout(height='auto', width='100%'), width=2000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotter = pv.Plotter(\n",
    "    lighting='three lights'\n",
    ")\n",
    "plotter.background_color = \"white\"\n",
    "\n",
    "# cpos = [(-57.05794118047339, -173.82419298093834, 190.51142547607827),\n",
    "#  (64.0, 64.0, 64.43975067138672),\n",
    "#  (-0.8136483842355717, 0.5348925173329734, 0.2277417434991626)]\n",
    "\n",
    "cpos = 'iso'\n",
    "sa_plane = pv.Plane(center=sa_support.tolist(), direction=sa_normal.tolist(), i_size=100, j_size=100, i_resolution=1, j_resolution=1)\n",
    "sa_plane.point_data.clear()\n",
    "sa_edges = sa_plane.extract_feature_edges(boundary_edges=True, feature_edges=False, manifold_edges=False)\n",
    "\n",
    "hla_plane = pv.Plane(center=hla_support.tolist(), direction=hla_normal.tolist(), i_size=100, j_size=100, i_resolution=1, j_resolution=1)\n",
    "hla_plane.point_data.clear()\n",
    "hla_edges = hla_plane.extract_feature_edges(boundary_edges=True, feature_edges=False, manifold_edges=False)\n",
    "\n",
    "full_mesh = []\n",
    "\n",
    "# Prepare meshes\n",
    "for idx, (tag, data) in enumerate(heart_data.items()):\n",
    "    surf = pv.PolyData(data['verts'].numpy(), data['pyvista_faces'].view(-1).numpy())\n",
    "    scalars=np.array([idx]*data['verts'].shape[0])\n",
    "    surf.point_data.set_scalars(scalars, 'scalars')\n",
    "    smooth = surf.smooth_taubin(n_iter=100, pass_band=0.3)\n",
    "    full_mesh.append(smooth)\n",
    "\n",
    "block = pv.MultiBlock(full_mesh)\n",
    "full_mesh = block.combine(merge_points=False)\n",
    "\n",
    "sa_slice = full_mesh.slice(normal=sa_normal.tolist(), origin=sa_support.tolist())\n",
    "\n",
    "plotter.add_mesh(full_mesh, name='all', cmap=palette, line_width=2, show_scalar_bar=False, smooth_shading=True)   \n",
    "plotter.add_mesh(sa_slice, name=tag +'_sa_slice', cmap=palette, line_width=2, show_scalar_bar=False)    \n",
    "plotter.add_mesh(sa_plane, color=palette[idx],  opacity=0.3, show_edges=False, line_width=2)\n",
    "plotter.add_mesh(sa_edges, color=dark_palette[idx], line_width=1)\n",
    "\n",
    "hla_slice = full_mesh.slice(normal=hla_normal.tolist(), origin=hla_support.tolist())\n",
    "\n",
    "plotter.add_mesh(hla_slice, name=tag +'_hla_slice', cmap=palette, line_width=2, show_scalar_bar=False)    \n",
    "plotter.add_mesh(hla_plane, color=palette[idx],  opacity=0.3, show_edges=False, line_width=2)\n",
    "plotter.add_mesh(hla_edges, color=dark_palette[idx], line_width=1)\n",
    "\n",
    "plotter.view_isometric()\n",
    "plotter.show(\n",
    "    window_size=[2000,2000], \n",
    "    # jupyter_backend='static',\n",
    "    cpos=cpos,\n",
    ")\n",
    "# pv.global_theme.anti_aliasing = 'ssaa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-74.39510547025466, -266.25200577202395, 160.91771143179494),\n",
       " (100.36705418048375, 86.35979157449216, 89.34951083603468),\n",
       " (-0.7767468638865815, 0.47005278688638497, 0.41918335723487804)]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotter.camera_position"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56a79d7869c25b13099d152cff1cf89590f05ef08fa886b34f910fa465e81fa1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
