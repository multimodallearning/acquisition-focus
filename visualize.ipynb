{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name               Util    Mem free  Cuda             User(s)\n",
      "----  --------------------  ------  ----------  ---------------  ---------\n",
      "   1  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   3  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   4  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   5  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   7  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   8  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "   9  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  10  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  11  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  12  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  13  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  14  Tesla V100-SXM3-32GB     0 %   32197 MiB  11.8(470.82.01)\n",
      "  15  Tesla V100-SXM3-32GB     0 %   22878 MiB  11.8(470.82.01)\n",
      "   2  Tesla V100-SXM3-32GB     0 %   22622 MiB  11.8(470.82.01)\n",
      "   0  Tesla V100-SXM3-32GB     0 %    1039 MiB  11.8(470.82.01)\n",
      "   6  Tesla V100-SXM3-32GB     0 %    1039 MiB  11.8(470.82.01)\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                 torch\n",
      "----  --------------------  --  -------\n",
      "   1  Tesla V100-SXM3-32GB  ->  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars('*'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_label_metrics, log_oa_metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import monai\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/data/output/worthy-resonance-122_best/\").glob(\"*.pth\")\n",
    "USE_EYE_AFFINE = True\n",
    "\n",
    "for pth_path in files:\n",
    "    data = torch.load(pth_path)\n",
    "    base_path = Path(pth_path.parent, f\"{data['batch']['id'][0]}\")\n",
    "    if USE_EYE_AFFINE:\n",
    "        sa_affine = np.eye(4)\n",
    "    else:\n",
    "        sa_affine = data['batch']['sa_affine'].squeeze(0)\n",
    "    nib.save(nib.Nifti1Image(data['input'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_input.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['target'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_target.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['output'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_output.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['batch']['image'].squeeze(0).cpu().float().numpy(), sa_affine), str(base_path)+\"_corresponding_input_image.nii.gz\")\n",
    "    # hla_affine = data['batch']['hla_affine'].squeeze(0)\n",
    "    # D_slc, H_slc = data['batch']['hla_label_slc'].squeeze(0).shape\n",
    "    # nib.save(nib.Nifti1Image(data['batch']['hla_label_slc'].squeeze(0).view(1, D_slc, H_slc).cpu().float().numpy(), hla_affine), str(base_path)+\"_hla_label_slice.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "                   # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 500,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    \"fov_mm\": [300.0,300.0,300.0],\n",
    "    \"fov_vox\": [128,128,128],\n",
    "    'lr': 1e-3,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 'best',\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS train images and labels... (['mr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15 images, 15 labels:  30%|███       | 9/30 [00:14<00:33,  1.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m      2\u001b[0m     training_dataset \u001b[39m=\u001b[39m MMWHSDataset(\n\u001b[1;32m      3\u001b[0m         config\u001b[39m.\u001b[39mdata_base_path,\n\u001b[1;32m      4\u001b[0m         state\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         debug\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mdebug\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m training_dataset\n\u001b[0;32m---> 29\u001b[0m training_dataset \u001b[39m=\u001b[39m prepare_data(config_dict)\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(config):\n\u001b[0;32m----> 2\u001b[0m     training_dataset \u001b[39m=\u001b[39m MMWHSDataset(\n\u001b[1;32m      3\u001b[0m         config\u001b[39m.\u001b[39;49mdata_base_path,\n\u001b[1;32m      4\u001b[0m         state\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m         load_func\u001b[39m=\u001b[39;49mload_data,\n\u001b[1;32m      6\u001b[0m         extract_slice_func\u001b[39m=\u001b[39;49mextract_2d_data,\n\u001b[1;32m      7\u001b[0m         modality\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmodality,\n\u001b[1;32m      8\u001b[0m         do_align_global\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m         use_binarized_labels\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m         do_resample\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m# Prior to cropping, resample image?\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m         crop_3d_region\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \u001b[39m# Crop or pad the images to these dimensions\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m         crop_around_3d_label_center\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mcrop_around_3d_label_center,\n\u001b[1;32m     13\u001b[0m         pre_interpolation_factor\u001b[39m=\u001b[39;49m\u001b[39m1.\u001b[39;49m, \u001b[39m# When getting the data, resize the data by this factor\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m         ensure_labeled_pairs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m# Only use fully labelled images (segmentation label available)\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m         use_2d_normal_to\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49muse_2d_normal_to, \u001b[39m# Use 2D slices cut normal to D,H,>W< dimensions\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m         crop_around_2d_label_center\u001b[39m=\u001b[39;49m[\u001b[39m128\u001b[39;49m,\u001b[39m128\u001b[39;49m],\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m         fov_mm \u001b[39m=\u001b[39;49m [\u001b[39m224.\u001b[39;49m,\u001b[39m224.\u001b[39;49m,\u001b[39m224.\u001b[39;49m],\n\u001b[1;32m     19\u001b[0m         fov_vox \u001b[39m=\u001b[39;49m [\u001b[39m128\u001b[39;49m,\u001b[39m128\u001b[39;49m,\u001b[39m128\u001b[39;49m],\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m         augment_angle_std\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m         device\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m     24\u001b[0m         debug\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mdebug\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m training_dataset\n",
      "File \u001b[0;32m/shared/slice_inflate/slice_inflate/datasets/mmwhs_dataset.py:51\u001b[0m, in \u001b[0;36mMMWHSDataset.__init__\u001b[0;34m(self, state, label_tags, *args, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mif\u001b[39;00m kwargs[\u001b[39m'\u001b[39m\u001b[39muse_binarized_labels\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     49\u001b[0m     label_tags\u001b[39m=\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbackground\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mforeground\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, state\u001b[39m=\u001b[39;49mstate, label_tags\u001b[39m=\u001b[39;49mlabel_tags, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/shared/slice_inflate/slice_inflate/datasets/hybrid_id_dataset.py:60\u001b[0m, in \u001b[0;36mHybridIdDataset.__init__\u001b[0;34m(self, base_dir, load_func, extract_slice_func, ensure_labeled_pairs, do_resample, resample_size, do_normalize, max_load_3d_num, crop_3d_region, crop_around_3d_label_center, modified_3d_label_override, crop_2d_slices_gt_num_threshold, prevent_disturbance, label_tags, use_2d_normal_to, crop_around_2d_label_center, pre_interpolation_factor, device, debug, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment_at_collate \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39m# Load base 3D data\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m all_3d_data_dict \u001b[39m=\u001b[39m load_func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attributes)\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attributes[\u001b[39m'\u001b[39m\u001b[39mimg_paths\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_paths \u001b[39m=\u001b[39m all_3d_data_dict\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mimg_paths\u001b[39m\u001b[39m'\u001b[39m, {})\n\u001b[1;32m     63\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attributes[\u001b[39m'\u001b[39m\u001b[39mlabel_paths\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_paths \u001b[39m=\u001b[39m all_3d_data_dict\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mlabel_paths\u001b[39m\u001b[39m'\u001b[39m, {})\n",
      "File \u001b[0;32m/shared/slice_inflate/slice_inflate/datasets/mmwhs_dataset.py:443\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(self_attributes)\u001b[0m\n\u001b[1;32m    441\u001b[0m     label_data_3d[_3d_id] \u001b[39m=\u001b[39m tmp\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    442\u001b[0m     oh \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(tmp\u001b[39m.\u001b[39mlong())\u001b[39m.\u001b[39mpermute(\u001b[39m3\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 443\u001b[0m     additional_data_3d[_3d_id][\u001b[39m'\u001b[39m\u001b[39mlabel_distance_map\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m calc_dist_map(oh\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mbool(), mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    445\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:  \u001b[39m# Normalize image to zero mean and unit std\u001b[39;00m\n",
      "File \u001b[0;32m/shared/slice_inflate/slice_inflate/utils/torch_utils.py:586\u001b[0m, in \u001b[0;36mcalc_dist_map\u001b[0;34m(onehot_seg, mode)\u001b[0m\n\u001b[1;32m    584\u001b[0m     dm[idx] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mas_tensor(distance(posmask))\n\u001b[1;32m    585\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 586\u001b[0m     dm[idx] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(distance(negmask))\n\u001b[1;32m    587\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msigned\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    588\u001b[0m     dm[idx] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(distance(negmask) \u001b[39m*\u001b[39m negmask \u001b[39m-\u001b[39m (distance(posmask) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m posmask)\n",
      "File \u001b[0;32m/shared/slice_inflate/.venv/lib/python3.9/site-packages/scipy/ndimage/_morphology.py:2277\u001b[0m, in \u001b[0;36mdistance_transform_edt\u001b[0;34m(input, sampling, return_distances, return_indices, distances, indices)\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2275\u001b[0m     ft \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mzeros((\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mndim,) \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mint32)\n\u001b[0;32m-> 2277\u001b[0m _nd_image\u001b[39m.\u001b[39;49meuclidean_feature_transform(\u001b[39minput\u001b[39;49m, sampling, ft)\n\u001b[1;32m   2278\u001b[0m \u001b[39m# if requested, calculate the distance transform\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \u001b[39mif\u001b[39;00m return_distances:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"train\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        use_binarized_labels=False,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=[128,128],\n",
    "\n",
    "        fov_mm = [224.,224.,224.],\n",
    "        fov_vox = [128,128,128],\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 4))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# # Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "# sp = 1.0\n",
    "# verts, faces, normals, values = measure.marching_cubes(first_class.cpu().numpy(), spacing=(sp,sp,sp), step_size=4)\n",
    "\n",
    "# mesh = Poly3DCollection(verts[faces])\n",
    "# mesh.set_edgecolor('k')\n",
    "# ax.add_collection3d(mesh)\n",
    "\n",
    "# ax.set_xlim(0, 128)\n",
    "# ax.set_ylim(0, 128)\n",
    "# ax.set_zlim(0, 128)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = training_dataset[1]\n",
    "label = sample['label']\n",
    "print(sample['id'])\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "heart_data = {}\n",
    "for class_idx, tag in enumerate(training_dataset.label_tags):\n",
    "    if class_idx == 0: continue\n",
    "\n",
    "    sub_label = label[class_idx, :,:,:]\n",
    "    verts, faces, normals, values = measure.marching_cubes(sub_label.cpu().numpy(), spacing=SPACING, step_size=STEP_SIZE)\n",
    "    data = dict(\n",
    "        verts=torch.as_tensor(verts.copy()),\n",
    "        faces=torch.as_tensor(faces.copy()),\n",
    "        normals=torch.as_tensor(normals.copy()), \n",
    "        values=torch.as_tensor(values.copy())\n",
    "    )\n",
    "    heart_data[tag] = data\n",
    "\n",
    "heart_data['sa_affine'] = sample['sa_affine']\n",
    "heart_data['hla_affine'] = sample['hla_affine']\n",
    "\n",
    "torch.save(heart_data, 'mmwhs_sample2_clouds.pth')\n",
    "nib.save(nib.Nifti1Image(sample['label'].argmax(0).cpu().int().numpy(), affine=sample['sa_affine'].detach().cpu().numpy()), \"mmwhs_sample2_sa_label.nii.gz\")\n",
    "nib.save(nib.Nifti1Image(sample['hla_label_slc'].argmax(0).cpu().int().numpy(), affine=sample['hla_affine'].detach().cpu().numpy()), \"mmwhs_sample2_hla_label_slc.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View full heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View sliced heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_plane(normal, support, point):\n",
    "    normal = normal / normal.dot(normal).sqrt() # Get unit vector\n",
    "    diff = point-support.to(dtype=normal.dtype)\n",
    "    dist = normal.dot(diff).abs()\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "SA_NORMAL = torch.tensor([0.,0.,1.])\n",
    "SUPPORT = torch.tensor([64.,64.,64.])\n",
    "\n",
    "# sa_to_hla = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "# hla_normal = (sa_to_hla @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "# hla_support = (sa_to_hla @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "hla_to_sa = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "sa_to_hla = hla_to_sa.inverse()\n",
    "\n",
    "hla_normal = (hla_to_sa @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = (hla_to_sa @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = hla_support\n",
    "print(hla_normal)\n",
    "print(hla_support)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "s=1\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    selected_verts = []\n",
    "    for normal, support in [(SA_NORMAL, SUPPORT), (hla_normal, hla_support)]:\n",
    "        selected_verts.extend([v for v in verts if get_distance_from_plane(normal, support, v) < 1.])\n",
    "    if len(selected_verts) > 0:\n",
    "        selected_verts = torch.stack(selected_verts)\n",
    "        ax.scatter(selected_verts[:,0], selected_verts[:,1], selected_verts[:,2], s=s, label=tag)\n",
    "    else:\n",
    "        ax.scatter([],[],[],s=s,label=tag)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame * 2\n",
    "    ax.view_init(0., angle)\n",
    "    \n",
    "plt.legend()\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=180, interval=50)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaperformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeformer_data = np.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/ShapeFormer/out.npy\")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "verts = shapeformer_data\n",
    "ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against mean shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS train images and labels... (['mr', 'ct'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 images, 30 labels: 100%|██████████| 60/60 [00:46<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (30)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n",
      "Loading MMWHS test images and labels... (['mr', 'ct'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10 images, 10 labels: 100%|██████████| 20/20 [00:15<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (10)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "PROJECT_NAME = \"slice_inflate\"\n",
    "\n",
    "from pathlib import Path\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir, in_notebook\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk # https://simpleitk.org/\n",
    "import numpy as np\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "config_dict = DotDict(dict(\n",
    "    state='train',\n",
    "    dataset='mmwhs',\n",
    "    modality='all',\n",
    "    use_2d_normal_to=None,\n",
    "    data_base_path=str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    crop_3d_region=None, #((0,128), (0,128), (0,128)), # dimension range in which 3D samples are cropped\n",
    "    crop_2d_slices_gt_num_threshold=0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "    crop_around_3d_label_center=(128,128,128), #(128,128,128),\n",
    "    crop_around_2d_label_center=(128,128),\n",
    "    fov_mm=(300.,300.,300.),\n",
    "    fov_vox=(196,196,196),\n",
    "    max_load_3d_num=None,\n",
    "    device='cuda',\n",
    "    debug=False,\n",
    "))\n",
    "\n",
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=config.state,\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        use_binarized_labels=False,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        fov_mm=config.fov_mm,\n",
    "        fov_vox=config.fov_vox,\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=config.crop_around_2d_label_center,\n",
    "        max_load_3d_num=config.max_load_3d_num,\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "training_dataset, test_dataset = None, None\n",
    "\n",
    "if training_dataset is None:\n",
    "    train_config = DotDict(config_dict.copy())\n",
    "    training_dataset = prepare_data(train_config)\n",
    "\n",
    "if test_dataset is None:\n",
    "    test_config = DotDict(config_dict.copy())\n",
    "    test_config['state'] = 'test'\n",
    "    test_dataset = prepare_data(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ids: ['1004-ct', '1004-mr', '1007-ct', '1007-mr', '1008-ct', '1008-mr', '1009-ct', '1009-mr', '1010-ct', '1010-mr', '1011-ct', '1011-mr', '1012-ct', '1012-mr', '1013-ct', '1013-mr', '1014-ct', '1014-mr', '1015-ct', '1015-mr', '1016-ct', '1016-mr', '1017-ct', '1017-mr', '1018-ct', '1018-mr', '1019-ct', '1019-mr', '1020-ct', '1020-mr']\n",
      "testing ids: ['1001-ct', '1001-mr', '1002-ct', '1002-mr', '1003-ct', '1003-mr', '1005-ct', '1005-mr', '1006-ct', '1006-mr']\n"
     ]
    }
   ],
   "source": [
    "print(\"training ids:\", [smp['id'] for smp in training_dataset])\n",
    "print(\"testing ids:\", [smp['id'] for smp in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.eval()\n",
    "train_samples = []\n",
    "for sample in training_dataset:\n",
    "    train_samples.append(sample['label'])\n",
    "\n",
    "train_samples = torch.stack(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 128, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_samples.shape)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(train_samples.sum(0)[:,:,64].cpu(), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, counts = train_samples.unique(return_counts=True, dim=0)\n",
    "if uniques.numel() > 1:\n",
    "    pass\n",
    "sorted_idxs = counts.argsort()\n",
    "resulting_label = uniques[sorted_idxs][0]\n",
    "# print(resulting_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bincount(tens, length, dim=-1):\n",
    "  \"\"\"Count the number of ocurrences of each value along an axis.\"\"\"\n",
    "  mask = (tens[...].unsqueeze(-1) == torch.arange(length, device=tens.device)) # Last dimension will be broadcasted\n",
    "  return mask.count_nonzero(dim=dim-1 if dim < 0 else dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE+0lEQVR4nO3dP04bQRTH8QH7DBQpKOgjWUJcgAhxApooXThGSo5BTcMJEJIvgJAs0VNQUOQMGKdAG9b2rndm/7Dze+/7aRIRx7j46uXNGMjearVaBUDM/tgvAGiDcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCFpGvvAs/2LIV8HEEII4f79NupxTFxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIIlxIiv7PS7DbyWLZ23M9zCa9PZdVhNtBn7HWPS8RV2NVgCQmbqKhpuyuz8fU3cbETfDV0aIeEzfC2MFufn4mMOHuNHawMbwe5FgVIImJWyPnacvqwMStlHO0BY+xljFxS3IPtoj1ZLF0f01GuII8B1tgVRBBrOsIF5JYFUL+uy22EW7mWBGquV8VmLaa3IebM6ZtPcKFJMKFJA5nGWJFaMbEzQzRxiHcjBBtPMLNBNGmcb/jPswmo93lEmt7TFxIch8u75xpch3u3etitH+uWRO6cbvj3r0u1n4NIYQ/f7+P82KQzOXELcdadnXwFK4Ongb//Ezb7txM3LpYqxTxDjGBibYfLicu9LmZuG2U1wb237wQbqSq3Tc2ZtaD/rkIN2W/TbEZc1XIRDsMF+F+lZhIn29mIYQQjn4uhn0xxhFuD378+v3xm9PqP3+53H537vlmRrwdcKsASUzcDv5P2paKtSEEVodU5sPt+2CWGmvVmlCF3TcNq0KClGhfLpfR0ZaVpzDqmZ+4fRhqyqI90+F2WRPa7K99Bcvu24xVAZJMh3v+bdbq73W9LegTO2810+G2WRXaRjvkXvt8MyPgDabDTZVjtKhm+nAWi2D1uJ+4Oe2zu3C7sM59uNBkclWIPZSNeVeb4vB6Et5Oj9c+Np0/fvnryInJiXv+bdZ4FaYULba5nLgWon07PXY9dU2GW6XLIYzbg/y4CTfVmLHGrgeep67JHRf2EW4FhWnrnYtwU78AHPkzGW75OkzlnTGkMRluge9csMv9rUJusb5cLqP3XK83CiEYn7iwy+TE3Xxfv05u0xbx3E7cnKNt+63tnpgLN3baQpu5cGNYmGaeD2YhGN1x61gIFh9cTlzoI1xIchMua4ItpsL1cqPg/WAWgpPDmdq05Usbm7kIV8muaJm0n0ytCvDDfLhqa0Idpu068+EqYbeNR7gCmLbbzBzOlK/COJClMxPudP4oF2/TakC09VgVIMl0uDnfKHAQ68bMqqCEnbY70xM3R0TbD8LNBNGmIVxIMrfjlq/FDq8nox/QYg5hTNt0psLN6S6XYIfFqjCApkk/nT8SbUd7q9VqFfPAs/2LoV9Lb+qm7lBrQ8qdLMHudv9+G/U4U6tCoRxHEfEQ0aa+iUC0/WFVgCSTE7esOLCVp2Pb6cuEzYf5cEP4jLcI6Wj+sUIUP4u2CLnr1w8Q6tcxeThr0teVGaH2z/XhrMnmBI4NmVDz4TLcEEJltISpg1sFSHI7cQtMWU1MXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEgiXEiK/rkKQE6YuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJBEuJD0D8Sxa7LjuaApAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bncount_along_samples = bincount(train_samples, train_samples.max()+1, dim=0)\n",
    "bncount_along_samples.shape\n",
    "\n",
    "majority_voted_train = bncount_along_samples.argmax(-1)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(majority_voted_train[:,:,64].cpu(), interpolation='none', vmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.eval()\n",
    "\n",
    "test_samples = []\n",
    "for sample in test_dataset:\n",
    "    test_samples.append(sample['label'])\n",
    "test_samples = torch.stack(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 128, 128, 128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(test_samples.shape)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(test_samples.sum(0)[:,:,64].cpu(), interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICE tensor(0.6775, device='cuda:0')\n",
      "IOU tensor(0.5233, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"DICE\", monai.metrics.compute_dice(\n",
    "    torch.nn.functional.one_hot(majority_voted_train).repeat(10,1,1,1,1).permute(0,4,1,2,3),\n",
    "    torch.nn.functional.one_hot(test_samples).permute(0,4,1,2,3)\n",
    ")[:,1:].mean())\n",
    "\n",
    "print(\"IOU\", monai.metrics.compute_iou(\n",
    "    torch.nn.functional.one_hot(majority_voted_train).repeat(10,1,1,1,1).permute(0,4,1,2,3),\n",
    "    torch.nn.functional.one_hot(test_samples).permute(0,4,1,2,3)\n",
    ")[:,1:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAYAAACvDDbuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFEElEQVR4nO3cPU4cWRSG4QPNGhxMQDC5pZYQG+gR8gpIrMnGy3DoZRA7YQUWkjeALLXk3MEEE3gNQE1glSmqq5pbP7fqfue8T4ShZSp4dXTqdtEnVVVVBog5XfsCgDEIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5LOUl94dXqd8zoAMzO7e7pNeh0TF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5IIF5KSP0of87jcP3Z+/367scv9o91vNwtfkSYm7oL6om3+7Nhr8IyJO5M5g6v/L6ZvP8KdIPd0JOB+rAqQxMQdiB20DEzcAYi2HEzcBGsH2/797LyEe9TawaIfqwIkEW6Hy/1j0dO25GtbCuG2KERR77gK15oLO67pBNAM9n67CX2TxsQV0Yw0crA1woUkwoWk8OEq7LesBofCh1s6ou1GuJBEuAVj2vbjHLdABPs6Ji4kEW5hmLZpCBeSCLcgTNt04cMtJZZSrkMFpworItbxwk9caAof7lrPKjBtpwkf7hoBEe10YXfcL//tD7738efb5S8Eo4ScuF3Rmpl9evPdPr35vuzFYJQwE7cv1i5d8c41jVkT5hFy4kLfSVVVVcoLr06vc19LdkOm7jFjpy/T9nV3T7dJrwuzKsypXiVSAybY+YUJd65p29TehbtCJto8QoSbI9ouqZH++Ly1P9/v816McyHCze2vv//59cWu++f/fjh8d454p+FUAZKYuBP8nrRHdE3b2o/P2xf/ZgKncx9ujv02JViz49F2YX1I5z7cOaUGazY82hrxpiHcBEOCNRsfba1eIQi4H+H2GBqr2fRg25i+/ThVgCTC7TBm2ubSPnnAL67DHXOiUFK06Oc63KGmRDv3fovjuDmz6VM2d7TcpB0KP3FLjxbdwocLTaHDVZm25zcbe9hdLPK7VLDjjrDkenB+8/yMbzPes6/fFruGErkMN+UYrIR3xl7TjLbtYXcROl6X4b77Y2tm/QEv/ewB5ucy3DmCXTPWY5O2KfLUDX1zBl1hwlV5Kzd12prFvkFzuSq0d1yVFQHp3E9clUmLYdyHOwTTVofLVaGW648asT4mLiSFD5dpq8lluA+7i6SHUkqMtsRrKpHLcFOoBxL5DNfMYbiqkxbDuAsXMRAuJLk+x21jRfCDiVuYlIdsot+YmQUKl2nrS6hVoXSvTVsm7bMQE5dp60+IcOEP4YpgTXjJfbgqa8KQP9mBs3C9ftoL0/YQpwor4sx2PFcTVwmrwTSEC0muw1W5MevDmtCPHXcFvEM2neuJWyKinQfhLoho50O4kORuxz37+u33GxHnNxuJjwtl0g7nKtxmtGsi2PxYFVZCtNO4mrhmh1O3Of1yrQ18pu3y3IVr9jKOrojnCnjo27ZEOx9WBUhyOXGbum7Yhp42THkghimbh/twzZ7jrSN62F1kfTqLWPMLEa5Z/piIdVlhwm2aet5LpOsLGa6ZvVgb2t+v14r6Z4RaHk4VIOmkqqoq5YVXp9e5rwWwu6fbpNcxcSGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCEp+W/OgJIwcSGJcCGJcCGJcCGJcCGJcCGJcCGJcCGJcCHpf6enaak+w1psAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_samples = torch.cat([train_samples, test_samples], dim=0)\n",
    "all_samples.shape\n",
    "\n",
    "all_bncount_along_samples = bincount(all_samples, train_samples.max()+1, dim=0)\n",
    "\n",
    "majority_voted_all = all_bncount_along_samples.argmax(-1)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(majority_voted_all[:,:,64].cpu(), interpolation='none', vmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice tensor(0.7065, device='cuda:0')\n",
      "IOU tensor(0.5625, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Dice\", monai.metrics.compute_dice(\n",
    "    torch.nn.functional.one_hot(majority_voted_all).repeat(40,1,1,1,1).permute(0,4,1,2,3),\n",
    "    torch.nn.functional.one_hot(all_samples).permute(0,4,1,2,3)\n",
    ")[:,1:].mean())\n",
    "\n",
    "print(\"IOU\", monai.metrics.compute_iou(\n",
    "    torch.nn.functional.one_hot(majority_voted_all).repeat(40,1,1,1,1).permute(0,4,1,2,3),\n",
    "    torch.nn.functional.one_hot(all_samples).permute(0,4,1,2,3)\n",
    ")[:,1:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_multilabel_staple_consensus(lbl_list):\n",
    "    staple_filter = sitk.MultiLabelSTAPLEImageFilter()\n",
    "    # sitk.ProcessObject.SetGlobalDefaultDebugOff()\n",
    "    staple_filter.SetMaximumNumberOfIterations(200)\n",
    "    staple_filter.SetLabelForUndecidedPixels(0)\n",
    "    sitk_moving_data = [sitk.GetImageFromArray(lbl.to_dense().numpy().astype(np.uint64)) for lbl in lbl_list]\n",
    "    \n",
    "    staple_out = staple_filter.Execute(sitk_moving_data)\n",
    "    consensus = torch.tensor(sitk.GetArrayFromImage(staple_out).astype(np.int64))\n",
    "\n",
    "    return consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stapled = calc_multilabel_staple_consensus([smp for smp in train_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stapled.unique())\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(stapled[:,:,64], interpolation='none', vmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"testset vs. stapled: {dice3d(torch.nn.functional.one_hot(stapled).repeat(10,1,1,1,1),torch.nn.functional.one_hot(test_samples), one_hot_torch_style=True)[:,1:].mean().item()*100:.3f}%\")\n",
    "print(f\"testset vs. majority_voted: {dice3d(torch.nn.functional.one_hot(majority_voted).repeat(10,1,1,1,1),torch.nn.functional.one_hot(test_samples), one_hot_torch_style=True)[:,1:].mean().item()*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_slice_inflate import run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from slice_inflate.utils import torch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_data = nib.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/data/MMWHS/ct_train/ct_train_1004_label.nii.gz\")\n",
    "\n",
    "torch_data = torch.as_tensor(nii_data.get_fdata())\n",
    "_in = (torch_data > 0).float()\n",
    "\n",
    "print(_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch_utils.calc_dist_map(_in.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_boundary = torch_utils.get_seg_boundary(_in.view(1,1,512,512,200).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg_boundary.squeeze()[:,:,100], interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((dt * seg_boundary.squeeze().float().roll(20,1)).abs()[:,:,100], interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da46a6541ad1ebd554a6d91c7d93891a249a9a187bd5c787406972a8a9c6b0a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
