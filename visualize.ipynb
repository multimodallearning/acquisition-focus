{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_label_metrics, log_oa_metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/data/output/worthy-resonance-122_best/\").glob(\"*.pth\")\n",
    "USE_EYE_AFFINE = True\n",
    "\n",
    "for pth_path in files:\n",
    "    data = torch.load(pth_path)\n",
    "    base_path = Path(pth_path.parent, f\"{data['batch']['id'][0]}\")\n",
    "    if USE_EYE_AFFINE:\n",
    "        sa_affine = np.eye(4)\n",
    "    else:\n",
    "        sa_affine = data['batch']['sa_affine'].squeeze(0)\n",
    "    nib.save(nib.Nifti1Image(data['input'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_input.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['target'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_target.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['output'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_output.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['batch']['image'].squeeze(0).cpu().float().numpy(), sa_affine), str(base_path)+\"_corresponding_input_image.nii.gz\")\n",
    "    # hla_affine = data['batch']['hla_affine'].squeeze(0)\n",
    "    # D_slc, H_slc = data['batch']['hla_label_slc'].squeeze(0).shape\n",
    "    # nib.save(nib.Nifti1Image(data['batch']['hla_label_slc'].squeeze(0).view(1, D_slc, H_slc).cpu().float().numpy(), hla_affine), str(base_path)+\"_hla_label_slice.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "                   # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 500,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    \"fov_mm\": [300.0,300.0,300.0],\n",
    "    \"fov_vox\": [128,128,128],\n",
    "    'lr': 1e-3,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 'best',\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"train\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=[128,128],\n",
    "\n",
    "        fov_mm = [224.,224.,224.],\n",
    "        fov_vox = [128,128,128],\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 4))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# # Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "# sp = 1.0\n",
    "# verts, faces, normals, values = measure.marching_cubes(first_class.cpu().numpy(), spacing=(sp,sp,sp), step_size=4)\n",
    "\n",
    "# mesh = Poly3DCollection(verts[faces])\n",
    "# mesh.set_edgecolor('k')\n",
    "# ax.add_collection3d(mesh)\n",
    "\n",
    "# ax.set_xlim(0, 128)\n",
    "# ax.set_ylim(0, 128)\n",
    "# ax.set_zlim(0, 128)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = training_dataset[1]\n",
    "label = sample['label']\n",
    "print(sample['id'])\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "heart_data = {}\n",
    "for class_idx, tag in enumerate(training_dataset.label_tags):\n",
    "    if class_idx == 0: continue\n",
    "\n",
    "    sub_label = label[class_idx, :,:,:]\n",
    "    verts, faces, normals, values = measure.marching_cubes(sub_label.cpu().numpy(), spacing=SPACING, step_size=STEP_SIZE)\n",
    "    data = dict(\n",
    "        verts=torch.as_tensor(verts.copy()),\n",
    "        faces=torch.as_tensor(faces.copy()),\n",
    "        normals=torch.as_tensor(normals.copy()), \n",
    "        values=torch.as_tensor(values.copy())\n",
    "    )\n",
    "    heart_data[tag] = data\n",
    "\n",
    "heart_data['sa_affine'] = sample['sa_affine']\n",
    "heart_data['hla_affine'] = sample['hla_affine']\n",
    "\n",
    "torch.save(heart_data, 'mmwhs_sample2_clouds.pth')\n",
    "nib.save(nib.Nifti1Image(sample['label'].argmax(0).cpu().int().numpy(), affine=sample['sa_affine'].detach().cpu().numpy()), \"mmwhs_sample2_sa_label.nii.gz\")\n",
    "nib.save(nib.Nifti1Image(sample['hla_label_slc'].argmax(0).cpu().int().numpy(), affine=sample['hla_affine'].detach().cpu().numpy()), \"mmwhs_sample2_hla_label_slc.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View full heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View sliced heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_plane(normal, support, point):\n",
    "    normal = normal / normal.dot(normal).sqrt() # Get unit vector\n",
    "    diff = point-support.to(dtype=normal.dtype)\n",
    "    dist = normal.dot(diff).abs()\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "SA_NORMAL = torch.tensor([0.,0.,1.])\n",
    "SUPPORT = torch.tensor([64.,64.,64.])\n",
    "\n",
    "# sa_to_hla = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "# hla_normal = (sa_to_hla @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "# hla_support = (sa_to_hla @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "hla_to_sa = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "sa_to_hla = hla_to_sa.inverse()\n",
    "\n",
    "hla_normal = (hla_to_sa @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = (hla_to_sa @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = hla_support\n",
    "print(hla_normal)\n",
    "print(hla_support)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "s=1\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    selected_verts = []\n",
    "    for normal, support in [(SA_NORMAL, SUPPORT), (hla_normal, hla_support)]:\n",
    "        selected_verts.extend([v for v in verts if get_distance_from_plane(normal, support, v) < 1.])\n",
    "    if len(selected_verts) > 0:\n",
    "        selected_verts = torch.stack(selected_verts)\n",
    "        ax.scatter(selected_verts[:,0], selected_verts[:,1], selected_verts[:,2], s=s, label=tag)\n",
    "    else:\n",
    "        ax.scatter([],[],[],s=s,label=tag)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame * 2\n",
    "    ax.view_init(0., angle)\n",
    "    \n",
    "plt.legend()\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=180, interval=50)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaperformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeformer_data = np.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/ShapeFormer/out.npy\")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "verts = shapeformer_data\n",
    "ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare against mean shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "PROJECT_NAME = \"slice_inflate\"\n",
    "\n",
    "from pathlib import Path\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir, in_notebook\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk # https://simpleitk.org/\n",
    "import numpy as np\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "config_dict = DotDict(dict(\n",
    "    state='train',\n",
    "    dataset='mmwhs',\n",
    "    modality='all',\n",
    "    use_2d_normal_to=None,\n",
    "    data_base_path=str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    crop_3d_region=None, #((0,128), (0,128), (0,128)), # dimension range in which 3D samples are cropped\n",
    "    crop_2d_slices_gt_num_threshold=0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "    crop_around_3d_label_center=(128,128,128), #(128,128,128),\n",
    "    crop_around_2d_label_center=(128,128),\n",
    "    fov_mm=(300.,300.,300.),\n",
    "    fov_vox=(196,196,196),\n",
    "    max_load_3d_num=None,\n",
    "    device='cuda',\n",
    "    debug=False,\n",
    "))\n",
    "\n",
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=config.state,\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        fov_mm=config.fov_mm,\n",
    "        fov_vox=config.fov_vox,\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=config.crop_around_2d_label_center,\n",
    "        max_load_3d_num=config.max_load_3d_num,\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "training_dataset, test_dataset = None, None\n",
    "\n",
    "if training_dataset is None:\n",
    "    train_config = DotDict(config_dict.copy())\n",
    "    training_dataset = prepare_data(train_config)\n",
    "\n",
    "if test_dataset is None:\n",
    "    test_config = DotDict(config_dict.copy())\n",
    "    test_config['state'] = 'test'\n",
    "    test_dataset = prepare_data(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training ids:\", [smp['id'] for smp in training_dataset])\n",
    "print(\"testing ids:\", [smp['id'] for smp in test_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.eval()\n",
    "train_samples = []\n",
    "for sample in training_dataset:\n",
    "    train_samples.append(sample['label'])\n",
    "\n",
    "train_samples = torch.stack(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_samples.shape)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(train_samples.sum(0)[:,:,64], interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, counts = train_samples.unique(return_counts=True, dim=0)\n",
    "if uniques.numel() > 1:\n",
    "    pass\n",
    "sorted_idxs = counts.argsort()\n",
    "resulting_label = uniques[sorted_idxs][0]\n",
    "# print(resulting_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bincount(tens, length, dim=-1):\n",
    "  \"\"\"Count the number of ocurrences of each value along an axis.\"\"\"\n",
    "  mask = (tens[...].unsqueeze(-1) == torch.arange(length)) # Last dimension will be broadcasted\n",
    "  return mask.count_nonzero(dim=dim-1 if dim < 0 else dim)\n",
    "\n",
    "bncount_along_samples = bincount(train_samples, train_samples.max()+1, dim=0)\n",
    "bncount_along_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_voted = bncount_along_samples.argmax(-1)\n",
    "majority_voted.shape\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(majority_voted[:,:,64], interpolation='none', vmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdl_seg_class.metrics import dice3d\n",
    "test_dataset.eval()\n",
    "\n",
    "test_samples = []\n",
    "for sample in test_dataset:\n",
    "    test_samples.append(sample['label'])\n",
    "\n",
    "test_samples = torch.stack(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_samples.shape)\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(test_samples.sum(0)[:,:,64], interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice3d(\n",
    "    torch.nn.functional.one_hot(majority_voted).repeat(10,1,1,1,1),\n",
    "    torch.nn.functional.one_hot(test_samples),\n",
    "    one_hot_torch_style=True\n",
    ")[:,1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_multilabel_staple_consensus(lbl_list):\n",
    "    staple_filter = sitk.MultiLabelSTAPLEImageFilter()\n",
    "    # sitk.ProcessObject.SetGlobalDefaultDebugOff()\n",
    "    staple_filter.SetMaximumNumberOfIterations(200)\n",
    "    staple_filter.SetLabelForUndecidedPixels(0)\n",
    "    sitk_moving_data = [sitk.GetImageFromArray(lbl.to_dense().numpy().astype(np.uint64)) for lbl in lbl_list]\n",
    "    \n",
    "    staple_out = staple_filter.Execute(sitk_moving_data)\n",
    "    consensus = torch.tensor(sitk.GetArrayFromImage(staple_out).astype(np.int64))\n",
    "\n",
    "    return consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stapled = calc_multilabel_staple_consensus([smp for smp in train_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stapled.unique())\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.axis('off')\n",
    "plt.imshow(stapled[:,:,64], interpolation='none', vmax=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"testset vs. stapled: {dice3d(torch.nn.functional.one_hot(stapled).repeat(10,1,1,1,1),torch.nn.functional.one_hot(test_samples), one_hot_torch_style=True)[:,1:].mean().item()*100:.3f}%\")\n",
    "print(f\"testset vs. majority_voted: {dice3d(torch.nn.functional.one_hot(majority_voted).repeat(10,1,1,1,1),torch.nn.functional.one_hot(test_samples), one_hot_torch_style=True)[:,1:].mean().item()*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_slice_inflate import run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import distance_transform_edt as distance\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from slice_inflate.utils import torch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_data = nib.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/data/MMWHS/ct_train/ct_train_1004_label.nii.gz\")\n",
    "\n",
    "torch_data = torch.as_tensor(nii_data.get_fdata())\n",
    "_in = (torch_data > 0).float()\n",
    "\n",
    "print(_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = torch_utils.calc_dist_map(_in.bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_boundary = torch_utils.get_seg_boundary(_in.view(1,1,512,512,200).bool())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(seg_boundary.squeeze()[:,:,100], interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow((dt * seg_boundary.squeeze().float().roll(20,1)).abs()[:,:,100], interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e8264c9cb9bb8a6cada87af39a6b7aa8c2638398580ca823279198d429a8d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
