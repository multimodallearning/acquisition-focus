{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_label_metrics, log_oa_metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(\"/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/data/output/worthy-resonance-122_best/\").glob(\"*.pth\")\n",
    "USE_EYE_AFFINE = True\n",
    "\n",
    "for pth_path in files:\n",
    "    data = torch.load(pth_path)\n",
    "    base_path = Path(pth_path.parent, f\"{data['batch']['id'][0]}\")\n",
    "    if USE_EYE_AFFINE:\n",
    "        sa_affine = np.eye(4)\n",
    "    else:\n",
    "        sa_affine = data['batch']['sa_affine'].squeeze(0)\n",
    "    nib.save(nib.Nifti1Image(data['input'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_input.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['target'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_target.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['output'].argmax(1).squeeze(0).cpu().int().numpy(), sa_affine), str(base_path)+\"_output.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(data['batch']['image'].squeeze(0).cpu().float().numpy(), sa_affine), str(base_path)+\"_corresponding_input_image.nii.gz\")\n",
    "    # hla_affine = data['batch']['hla_affine'].squeeze(0)\n",
    "    # D_slc, H_slc = data['batch']['hla_label_slc'].squeeze(0).shape\n",
    "    # nib.save(nib.Nifti1Image(data['batch']['hla_label_slc'].squeeze(0).view(1, D_slc, H_slc).cpu().float().numpy(), hla_affine), str(base_path)+\"_hla_label_slice.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "                   # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 500,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    'lr': 1e-3,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 'best',\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS train images and labels... (['mr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15 images, 15 labels: 100%|██████████| 30/30 [00:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (15)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"train\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=(128,128),\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 4))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# # Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "# sp = 1.0\n",
    "# verts, faces, normals, values = measure.marching_cubes(first_class.cpu().numpy(), spacing=(sp,sp,sp), step_size=4)\n",
    "\n",
    "# mesh = Poly3DCollection(verts[faces])\n",
    "# mesh.set_edgecolor('k')\n",
    "# ax.add_collection3d(mesh)\n",
    "\n",
    "# ax.set_xlim(0, 128)\n",
    "# ax.set_ylim(0, 128)\n",
    "# ax.set_zlim(0, 128)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007-mr\n"
     ]
    }
   ],
   "source": [
    "sample = training_dataset[1]\n",
    "label = torch.nn.functional.one_hot(sample['label'], len(training_dataset.label_tags))\n",
    "print(sample['id'])\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "heart_data = {}\n",
    "for class_idx, tag in enumerate(training_dataset.label_tags):\n",
    "    if class_idx == 0: continue\n",
    "\n",
    "    sub_label = label[:,:,:,class_idx]\n",
    "    verts, faces, normals, values = measure.marching_cubes(sub_label.cpu().numpy(), spacing=SPACING, step_size=STEP_SIZE)\n",
    "    data = dict(\n",
    "        verts=torch.as_tensor(verts.copy()),\n",
    "        faces=torch.as_tensor(faces.copy()),\n",
    "        normals=torch.as_tensor(normals.copy()), \n",
    "        values=torch.as_tensor(values.copy())\n",
    "    )\n",
    "    heart_data[tag] = data\n",
    "\n",
    "heart_data['sa_affine'] = sample['sa_affine']\n",
    "heart_data['hla_affine'] = sample['hla_affine']\n",
    "\n",
    "torch.save(heart_data, 'mmwhs_sample2_clouds.pth')\n",
    "nib.save(nib.Nifti1Image(sample['label'].int().numpy(), affine=sample['sa_affine'].numpy()), \"mmwhs_sample2_sa_label.nii.gz\")\n",
    "nib.save(nib.Nifti1Image(sample['hla_label_slc'].int().numpy(), affine=sample['hla_affine'].numpy()), \"mmwhs_sample2_hla_label_slc.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View full heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View sliced heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_plane(normal, support, point):\n",
    "    normal = normal / normal.dot(normal).sqrt() # Get unit vector\n",
    "    diff = point-support.to(dtype=normal.dtype)\n",
    "    dist = normal.dot(diff).abs()\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "SA_NORMAL = torch.tensor([0.,0.,1.])\n",
    "SUPPORT = torch.tensor([64.,64.,64.])\n",
    "\n",
    "# sa_to_hla = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "# hla_normal = (sa_to_hla @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "# hla_support = (sa_to_hla @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "hla_to_sa = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "sa_to_hla = hla_to_sa.inverse()\n",
    "\n",
    "hla_normal = (hla_to_sa @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = (hla_to_sa @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = hla_support\n",
    "print(hla_normal)\n",
    "print(hla_support)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "s=1\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    selected_verts = []\n",
    "    for normal, support in [(SA_NORMAL, SUPPORT), (hla_normal, hla_support)]:\n",
    "        selected_verts.extend([v for v in verts if get_distance_from_plane(normal, support, v) < 1.])\n",
    "    if len(selected_verts) > 0:\n",
    "        selected_verts = torch.stack(selected_verts)\n",
    "        ax.scatter(selected_verts[:,0], selected_verts[:,1], selected_verts[:,2], s=s, label=tag)\n",
    "    else:\n",
    "        ax.scatter([],[],[],s=s,label=tag)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame * 2\n",
    "    ax.view_init(0., angle)\n",
    "    \n",
    "plt.legend()\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=180, interval=50)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaperformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeformer_data = np.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/ShapeFormer/out.npy\")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "verts = shapeformer_data\n",
    "ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e8264c9cb9bb8a6cada87af39a6b7aa8c2638398580ca823279198d429a8d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
