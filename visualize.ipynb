{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, get_batch_dice_over_all, get_batch_dice_per_label, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_label_metrics, log_oa_metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "import numpy as np\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "from skimage import measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,                      # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 100,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    'lr': 0.001,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 'best',\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendowskiAE(torch.nn.Module):\n",
    "\n",
    "    class ConvBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels: int, out_channels_list: list, strides_list: list):\n",
    "            super().__init__()\n",
    "\n",
    "            ops = []\n",
    "            in_channels = [in_channels] + out_channels_list[:-1]\n",
    "            for op_idx in range(len(out_channels_list)):\n",
    "                ops.append(torch.nn.Conv3d(\n",
    "                    in_channels[op_idx],\n",
    "                    out_channels_list[op_idx],\n",
    "                    kernel_size=3,\n",
    "                    stride=strides_list[op_idx],\n",
    "                    padding=1\n",
    "                ))\n",
    "\n",
    "            self.block = torch.nn.Sequential(*ops)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, decoder_in_channels=2, debug_mode=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        self.first_layer_encoder = self.ConvBlock(in_channels, out_channels_list=[8], strides_list=[1])\n",
    "        self.first_layer_decoder = self.ConvBlock(8, out_channels_list=[8,out_channels], strides_list=[1,1])\n",
    "\n",
    "        self.second_layer_encoder = self.ConvBlock(8, out_channels_list=[20,20,20], strides_list=[2,1,1])\n",
    "        self.second_layer_decoder = self.ConvBlock(20, out_channels_list=[8], strides_list=[1])\n",
    "\n",
    "        self.third_layer_encoder = self.ConvBlock(20, out_channels_list=[40,40,40], strides_list=[2,1,1])\n",
    "        self.third_layer_decoder = self.ConvBlock(40, out_channels_list=[20], strides_list=[1])\n",
    "\n",
    "        self.fourth_layer_encoder = self.ConvBlock(40, out_channels_list=[60,60,60], strides_list=[2,1,1])\n",
    "        self.fourth_layer_decoder = self.ConvBlock(decoder_in_channels, out_channels_list=[40], strides_list=[1])\n",
    "\n",
    "        self.fifth_layer = self.ConvBlock(60, out_channels_list=[60,20,2], strides_list=[2,1,1])\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            self.first_layer_encoder,\n",
    "            self.second_layer_encoder,\n",
    "            self.third_layer_encoder,\n",
    "            self.fourth_layer_encoder,\n",
    "            self.fifth_layer\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.fourth_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.third_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.second_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.first_layer_decoder,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        if self.debug_mode:\n",
    "            return debug_forward_pass(self.encoder, x, STEP_MODE=False)\n",
    "        else:\n",
    "            return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        if self.debug_mode:\n",
    "            return debug_forward_pass(self.decoder, z, STEP_MODE=False)\n",
    "        else:\n",
    "            return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "\n",
    "\n",
    "class BlendowskiVAE(BlendowskiAE):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['decoder_in_channels'] = 1\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def sample_z(self, mean, std):\n",
    "        return torch.normal(mean=mean, std=std).unsqueeze(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encode(x)\n",
    "        z = self.sample_z(h[:,0], (h[:,1]**2).sqrt())\n",
    "        return self.decode(z), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    model = BlendowskiAE(in_channels=num_classes, out_channels=num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"Param count model: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=20, verbose=True)\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading model, optimizers and grad scalers from {_path}\")\n",
    "        model.load_state_dict(torch.load(_path.joinpath('model.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scheduler.load_state_dict(torch.load(_path.joinpath('scheduler.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(f\"Generating fresh '{type(model).__name__}' model, optimizer and grad scaler.\")\n",
    "\n",
    "    return (model, optimizer, scheduler, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"training\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=(128,128),\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"data/models/solar-vortex-7_fold0_best\"\n",
    "(model, optimizer, scheduler, scaler) = get_model(config_dict, len(training_dataset), len(training_dataset.label_tags),\n",
    "    THIS_SCRIPT_DIR=Path(THIS_SCRIPT_DIR), _path=model_path, device=config_dict.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_from_sample(sample, config, num_classes):\n",
    "    b_hla_slc_seg = sample['hla_label_slc'].unsqueeze(0)\n",
    "    b_sa_slc_seg = sample['sa_label_slc'].unsqueeze(0)\n",
    "    b_input = torch.cat(\n",
    "        [b_sa_slc_seg.unsqueeze(1).repeat(1,64,1,1),\n",
    "            b_hla_slc_seg.unsqueeze(1).repeat(1,64,1,1)],\n",
    "            dim=1\n",
    "    )\n",
    "    b_seg = sample['label'].unsqueeze(0)\n",
    "\n",
    "    b_input = b_input.to(device=config.device)\n",
    "    b_seg = b_seg.to(device=config.device)\n",
    "\n",
    "    b_input = F.one_hot(b_input, num_classes).permute(0,4,1,2,3)\n",
    "    b_input = b_input.float()\n",
    "\n",
    "    return b_input, b_seg\n",
    "\n",
    "def inference_wrap(model, b_seg):\n",
    "    with torch.inference_mode():\n",
    "        b_out = model(b_seg)[0]\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset.eval()\n",
    "\n",
    "for sample in [training_dataset[_id] for _id in ['1001-mr', '1002-mr', '1003-mr', '1004-mr']]:\n",
    "    b_input, b_seg = get_model_input_from_sample(sample, config_dict, len(training_dataset.label_tags))\n",
    "    b_output = inference_wrap(model, b_input)\n",
    "\n",
    "    nib.save(nib.Nifti1Image(b_output.squeeze(0).cpu().int().numpy(), np.eye(4,4)), f\"{sample['id']}_pred.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(sample['label'].cpu().int().numpy(), np.eye(4,4)), f\"{sample['id']}_target.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(sample['image'].cpu().numpy(), np.eye(4,4)), f\"{sample['id']}_image.nii.gz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "                   # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 500,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    'lr': 1e-3,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 'best',\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS train images and labels... (['mr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15 images, 15 labels: 100%|██████████| 30/30 [00:13<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (15)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"train\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=(128,128),\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset\n",
    "\n",
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(4, 4))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# # Use marching cubes to obtain the surface mesh of these ellipsoids\n",
    "# sp = 1.0\n",
    "# verts, faces, normals, values = measure.marching_cubes(first_class.cpu().numpy(), spacing=(sp,sp,sp), step_size=4)\n",
    "\n",
    "# mesh = Poly3DCollection(verts[faces])\n",
    "# mesh.set_edgecolor('k')\n",
    "# ax.add_collection3d(mesh)\n",
    "\n",
    "# ax.set_xlim(0, 128)\n",
    "# ax.set_ylim(0, 128)\n",
    "# ax.set_zlim(0, 128)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1007-mr\n"
     ]
    }
   ],
   "source": [
    "sample = training_dataset[1]\n",
    "label = torch.nn.functional.one_hot(sample['label'], len(training_dataset.label_tags))\n",
    "print(sample['id'])\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "heart_data = {}\n",
    "for class_idx, tag in enumerate(training_dataset.label_tags):\n",
    "    if class_idx == 0: continue\n",
    "\n",
    "    sub_label = label[:,:,:,class_idx]\n",
    "    verts, faces, normals, values = measure.marching_cubes(sub_label.cpu().numpy(), spacing=SPACING, step_size=STEP_SIZE)\n",
    "    data = dict(\n",
    "        verts=torch.as_tensor(verts.copy()),\n",
    "        faces=torch.as_tensor(faces.copy()),\n",
    "        normals=torch.as_tensor(normals.copy()), \n",
    "        values=torch.as_tensor(values.copy())\n",
    "    )\n",
    "    heart_data[tag] = data\n",
    "\n",
    "heart_data['sa_affine'] = sample['sa_affine']\n",
    "heart_data['hla_affine'] = sample['hla_affine']\n",
    "\n",
    "torch.save(heart_data, 'mmwhs_sample2_clouds.pth')\n",
    "nib.save(nib.Nifti1Image(sample['label'].int().numpy(), affine=sample['sa_affine'].numpy()), \"mmwhs_sample2_sa_label.nii.gz\")\n",
    "nib.save(nib.Nifti1Image(sample['hla_label_slc'].int().numpy(), affine=sample['hla_affine'].numpy()), \"mmwhs_sample2_hla_label_slc.nii.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View full heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View sliced heart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_plane(normal, support, point):\n",
    "    normal = normal / normal.dot(normal).sqrt() # Get unit vector\n",
    "    diff = point-support.to(dtype=normal.dtype)\n",
    "    dist = normal.dot(diff).abs()\n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = torch.load('mmwhs_sample2_clouds.pth')\n",
    "\n",
    "SA_NORMAL = torch.tensor([0.,0.,1.])\n",
    "SUPPORT = torch.tensor([64.,64.,64.])\n",
    "\n",
    "# sa_to_hla = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "# hla_normal = (sa_to_hla @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "# hla_support = (sa_to_hla @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3].flip(0)\n",
    "hla_to_sa = heart_data['sa_affine'].inverse() @ heart_data['hla_affine']\n",
    "sa_to_hla = hla_to_sa.inverse()\n",
    "\n",
    "hla_normal = (hla_to_sa @ torch.tensor([0.,0.,1.,0.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = (hla_to_sa @ torch.tensor([64.,64.,64.,1.]).to(dtype=sa_to_hla.dtype))[:3]\n",
    "hla_support = hla_support\n",
    "print(hla_normal)\n",
    "print(hla_support)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.set_xlim(0, 128)\n",
    "ax.set_ylim(0, 128)\n",
    "ax.set_zlim(0, 128)\n",
    "s=1\n",
    "\n",
    "for tag, tag_data in heart_data.items():\n",
    "    if 'affine' in tag: continue\n",
    "    verts = tag_data['verts']\n",
    "    selected_verts = []\n",
    "    for normal, support in [(SA_NORMAL, SUPPORT), (hla_normal, hla_support)]:\n",
    "        selected_verts.extend([v for v in verts if get_distance_from_plane(normal, support, v) < 1.])\n",
    "    if len(selected_verts) > 0:\n",
    "        selected_verts = torch.stack(selected_verts)\n",
    "        ax.scatter(selected_verts[:,0], selected_verts[:,1], selected_verts[:,2], s=s, label=tag)\n",
    "    else:\n",
    "        ax.scatter([],[],[],s=s,label=tag)\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame * 2\n",
    "    ax.view_init(0., angle)\n",
    "    \n",
    "plt.legend()\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=180, interval=50)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaperformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapeformer_data = np.load(\"/share/data_supergrover1/weihsbach/shared_data/tmp/ShapeFormer/out.npy\")\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "SPACING = (1,1,1)\n",
    "STEP_SIZE = 2\n",
    "\n",
    "verts = shapeformer_data\n",
    "ax.scatter(verts[:,0], verts[:,1], verts[:,2], s=1)\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def anim_func(frame):\n",
    "    angle = frame\n",
    "    ax.view_init(30, angle)\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = \"/home/weihsbach/miniconda3/envs/binaries/bin/ffmpeg\"\n",
    "\n",
    "anim_created = FuncAnimation(fig, anim_func, frames=360, interval=25)\n",
    "display.display(display.HTML(anim_created.to_html5_video()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50e8264c9cb9bb8a6cada87af39a6b7aa8c2638398580ca823279198d429a8d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
