{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1294502f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, get_batch_dice_over_all, get_batch_dice_per_class, save_model\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_class_dices\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d\n",
    "import numpy as np\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "PROJECT_NAME = \"slice_inflate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3fce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config_dict = DotDict({\n",
    "    'num_folds': 5,\n",
    "    'only_first_fold': True,                # If true do not contiue with training after the first fold\n",
    "    # 'fold_override': 0,\n",
    "    # 'checkpoint_epx': 0,\n",
    "\n",
    "    'use_mind': False,                      # If true use MIND features (https://pubmed.ncbi.nlm.nih.gov/22722056/)\n",
    "    'epochs': 1000,\n",
    "\n",
    "    'batch_size': 4,\n",
    "    'val_batch_size': 1,\n",
    "    'modality': 'mr',\n",
    "    'use_2d_normal_to': None,               # Can be None or 'D', 'H', 'W'. If not None 2D slices will be selected for training\n",
    "\n",
    "    'dataset': 'mmwhs',                 # The dataset prepared with our preprocessing scripts\n",
    "    'data_base_path': str(Path(THIS_SCRIPT_DIR, \"data/MMWHS\")),\n",
    "    'reg_state': None, # Registered (noisy) labels used in training. See prepare_data() for valid reg_states\n",
    "    'train_set_max_len': None,              # Length to cut of dataloader sample count\n",
    "    'crop_around_3d_label_center': (128,128,128),\n",
    "    'crop_3d_region': ((0,128), (0,128), (0,128)),        # dimension range in which 3D samples are cropped\n",
    "    'crop_2d_slices_gt_num_threshold': 0,   # Drop 2D slices if less than threshold pixels are positive\n",
    "\n",
    "    'lr': 0.001,\n",
    "    'use_scheduling': True,\n",
    "\n",
    "    'save_every': 500,\n",
    "    'mdl_save_prefix': 'data/models',\n",
    "\n",
    "    'debug': False,\n",
    "    'wandb_mode': 'online',                         # e.g. online, disabled. Use weights and biases online logging\n",
    "    'do_sweep': False,                                # Run multiple trainings with varying config values defined in sweep_config_dict below\n",
    "\n",
    "    # For a snapshot file: dummy-a2p2z76CxhCtwLJApfe8xD_fold0_epx0\n",
    "    'checkpoint_name': None,                          # Training snapshot name, e.g. dummy-a2p2z76CxhCtwLJApfe8xD\n",
    "    'fold_override': None,                            # Training fold, e.g. 0\n",
    "    'checkpoint_epx': None,                           # Training epx, e.g. 0\n",
    "\n",
    "    'do_plot': False,                                 # Generate plots (debugging purpose)\n",
    "    'save_dp_figures': False,                         # Plot data parameter value distribution\n",
    "    'save_labels': True,                              # Store training labels alongside data parameter values inside the training snapshot\n",
    "\n",
    "    'device': 'cuda'\n",
    "})\n",
    "\n",
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=\"training\",\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=(128,128),\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "447c2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    training_dataset.train(augment=False)\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 2\n",
    "    print(training_dataset.do_augment)\n",
    "    for sample in [training_dataset[idx] for idx in [1]]:\n",
    "        pass\n",
    "        fig = plt.figure(figsize=(16., 4.))\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "            nrows_ncols=(1, 6),  # creates 2x2 grid of axes\n",
    "            axes_pad=0.0,  # pad between axes in inch.\n",
    "        )\n",
    "\n",
    "        show_row = [\n",
    "            cut_slice(sample['image']),\n",
    "            cut_slice(sample['label']),\n",
    "\n",
    "            sample['sa_image_slc'],\n",
    "            sample['sa_label_slc'],\n",
    "\n",
    "            sample['hla_image_slc'],\n",
    "            sample['hla_label_slc'],\n",
    "        ]\n",
    "\n",
    "        for ax, im in zip(grid, show_row):\n",
    "            ax.imshow(im, cmap='gray', interpolation='none')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c0b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    training_dataset.train()\n",
    "\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 10\n",
    "    print(training_dataset.do_augment)\n",
    "    import torch\n",
    "    lbl, sa_label, hla_label = torch.zeros(128,128), torch.zeros(128,128), torch.zeros(128,128)\n",
    "    for idx in range(15):\n",
    "        sample = training_dataset[1]\n",
    "        # nib.save(nib.Nifti1Image(sample['label'].cpu().numpy(), affine=torch.eye(4).numpy()), f'out{idx}.nii.gz')\n",
    "        lbl += cut_slice(sample['label']).cpu()\n",
    "        sa_label += sample['sa_label_slc'].cpu()\n",
    "        hla_label += sample['hla_label_slc'].cpu()\n",
    "    fig = plt.figure(figsize=(16., 4.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "        axes_pad=0.0,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    show_row = [\n",
    "        lbl, sa_label, hla_label\n",
    "    ]\n",
    "\n",
    "    for ax, im in zip(grid, show_row):\n",
    "        ax.imshow(im, cmap='magma', interpolation='none')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9d0a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset = prepare_data(config_dict)\n",
    "    training_dataset.train(augment=False)\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 2\n",
    "    print(training_dataset.do_augment)\n",
    "\n",
    "    lbl, sa_label, hla_label = torch.zeros(128,128), torch.zeros(128,128), torch.zeros(128,128)\n",
    "    for tr_idx in range(len(training_dataset)):\n",
    "        sample = training_dataset[tr_idx]\n",
    "\n",
    "        lbl += cut_slice(sample['label']).cpu()\n",
    "        sa_label += sample['sa_label_slc'].cpu()\n",
    "        hla_label += sample['hla_label_slc'].cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(16., 4.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "        axes_pad=0.0,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    show_row = [\n",
    "        lbl, sa_label, hla_label\n",
    "    ]\n",
    "\n",
    "    for ax, im in zip(grid, show_row):\n",
    "        ax.imshow(im, cmap='magma', interpolation='none')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d793b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "def get_named_layers_leaves(module):\n",
    "    \"\"\" Returns all leaf layers of a pytorch module and a keychain as identifier.\n",
    "        e.g.\n",
    "        ...\n",
    "        ('features.0.5', nn.ReLU())\n",
    "        ...\n",
    "        ('classifier.0', nn.BatchNorm2D())\n",
    "        ('classifier.1', nn.Linear())\n",
    "    \"\"\"\n",
    "\n",
    "    return [(keychain, sub_mod) for keychain, sub_mod in list(module.named_modules()) if not next(sub_mod.children(), None)]\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def temp_forward_hooks(modules, pre_fwd_hook_fn=None, post_fwd_hook_fn=None):\n",
    "    handles = []\n",
    "    if pre_fwd_hook_fn:\n",
    "        handles.extend([mod.register_forward_pre_hook(pre_fwd_hook_fn) for mod in modules])\n",
    "    if post_fwd_hook_fn:\n",
    "        handles.extend([mod.register_forward_hook(post_fwd_hook_fn) for mod in modules])\n",
    "\n",
    "    yield\n",
    "    for hand in handles:\n",
    "        hand.remove()\n",
    "        \n",
    "def debug_forward_pass(module, inpt, STEP_MODE=False):\n",
    "    named_leaves = get_named_layers_leaves(module)\n",
    "    leave_mod_dict = {mod:keychain for keychain, mod in named_leaves}\n",
    "\n",
    "    def get_shape_str(interface_var):\n",
    "        if isinstance(interface_var, tuple):\n",
    "            shps = [str(elem.shape) if isinstance(elem, torch.Tensor) else type(elem) for elem in interface_var]\n",
    "            return ', '.join(shps)\n",
    "        elif isinstance(interface_var, torch.Tensor):\n",
    "            return interface_var.shape\n",
    "        return type(interface_var)\n",
    "\n",
    "    def print_pre_info(module, inpt):\n",
    "        inpt_shapes = get_shape_str(inpt)\n",
    "        print(f\"in:  {inpt_shapes}\")\n",
    "        print(f\"key: {leave_mod_dict[module]}\")\n",
    "        print(f\"mod: {module}\")\n",
    "        if STEP_MODE:\n",
    "            input(\"To continue forward pass press [ENTER]\")\n",
    "\n",
    "    def print_post_info(module, inpt, output):\n",
    "        output_shapes = get_shape_str(output)\n",
    "        print(f\"out: {output_shapes}\\n\")\n",
    "\n",
    "    with temp_forward_hooks(leave_mod_dict.keys(), print_pre_info, print_post_info):\n",
    "        return module(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "410ec046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendowskiAE(torch.nn.Module):\n",
    "\n",
    "    class ConvBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels: int, out_channels_list: list, strides_list: list):\n",
    "            super().__init__()\n",
    "\n",
    "            ops = []\n",
    "            in_channels = [in_channels] + out_channels_list[:-1]\n",
    "            for op_idx in range(len(out_channels_list)):\n",
    "                ops.append(torch.nn.Conv3d(\n",
    "                    in_channels[op_idx], \n",
    "                    out_channels_list[op_idx], \n",
    "                    kernel_size=3, \n",
    "                    stride=strides_list[op_idx],\n",
    "                    padding=1\n",
    "                ))\n",
    "\n",
    "            self.block = torch.nn.Sequential(*ops)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "\n",
    "        \n",
    "\n",
    "    def __init__(self, in_channels, out_channels, decoder_in_channels=2, debug_mode=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        self.first_layer_encoder = self.ConvBlock(in_channels, out_channels_list=[8], strides_list=[1])\n",
    "        self.first_layer_decoder = self.ConvBlock(8, out_channels_list=[8,out_channels], strides_list=[1,1])\n",
    "\n",
    "        self.second_layer_encoder = self.ConvBlock(8, out_channels_list=[20,20,20], strides_list=[2,1,1])\n",
    "        self.second_layer_decoder = self.ConvBlock(20, out_channels_list=[8], strides_list=[1])\n",
    "\n",
    "        self.third_layer_encoder = self.ConvBlock(20, out_channels_list=[40,40,40], strides_list=[2,1,1])\n",
    "        self.third_layer_decoder = self.ConvBlock(40, out_channels_list=[20], strides_list=[1])\n",
    "\n",
    "        self.fourth_layer_encoder = self.ConvBlock(40, out_channels_list=[60,60,60], strides_list=[2,1,1])\n",
    "        self.fourth_layer_decoder = self.ConvBlock(decoder_in_channels, out_channels_list=[40], strides_list=[1])\n",
    "\n",
    "        self.fifth_layer = self.ConvBlock(60, out_channels_list=[60,20,2], strides_list=[2,1,1])\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            self.first_layer_encoder,\n",
    "            self.second_layer_encoder,\n",
    "            self.third_layer_encoder,\n",
    "            self.fourth_layer_encoder,\n",
    "            self.fifth_layer\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.fourth_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.third_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.second_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.first_layer_decoder,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        if self.debug_mode:\n",
    "            return debug_forward_pass(self.encoder, x, STEP_MODE=False)\n",
    "        else:\n",
    "            return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        if self.debug_mode:\n",
    "            return debug_forward_pass(self.decoder, z, STEP_MODE=False)\n",
    "        else:\n",
    "            return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "        \n",
    "\n",
    "\n",
    "class BlendowskiVAE(BlendowskiAE):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs['decoder_in_channels'] = 1\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def sample_z(self, mean, std):\n",
    "        return torch.normal(mean=mean, std=std).unsqueeze(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encode(x)\n",
    "        z = self.sample_z(h[:,0], (h[:,1]**2).sqrt())\n",
    "        return self.decode(z), z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a063e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.zeros(1,8,128,128,128)\n",
    "# bae = BlendowskiAE(in_channels=8, out_channels=8)\n",
    "\n",
    "# y, z = bae(x)\n",
    "\n",
    "# print(\"BAE\")\n",
    "# print(\"x\", x.shape)\n",
    "# print(\"z\", z.shape)\n",
    "# print(\"y\", y.shape)\n",
    "# print()\n",
    "\n",
    "# bvae = BlendowskiVAE(in_channels=8, out_channels=8)\n",
    "\n",
    "# y, z = bvae(x)\n",
    "\n",
    "# print(\"BVAE\")\n",
    "# print(\"x\", x.shape)\n",
    "# print(\"z\", z.shape)\n",
    "# print(\"y\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7de04e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BlendowskiVAE(in_channels=6, out_channels=6)\n",
    "# model.cuda()\n",
    "# with torch.no_grad():\n",
    "#     smp = torch.nn.functional.one_hot(training_dataset[1]['label'], 6).unsqueeze(0).permute([0,4,1,2,3]).float().cuda()\n",
    "# y, _ = model(smp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c9dfb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS training images and labels... (['mr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 images, 20 labels: 100%|██████████| 40/40 [00:27<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (20)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "training_dataset = prepare_data(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af1cf84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dummy-EzxVz3eRm8H7haihXtEusU\n",
      "Will run validation with these 3D samples (#4): ['1001-mr', '1002-mr', '1003-mr', '1004-mr']\n",
      "Param count model: 556732\n",
      "Generating fresh 'BlendowskiAE' model, optimizer and grad scaler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 0\n",
      "### Training\n",
      "losses/loss_fold0 3.6996378898620605\n",
      "losses/loss_fold0 3.6996378898620605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReduceLROnPlateau' object has no attribute 'get_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 364>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=384'>385</a>\u001b[0m     wandb\u001b[39m.\u001b[39magent(sweep_id, function\u001b[39m=\u001b[39msweep_run)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=386'>387</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=387'>388</a>\u001b[0m     normal_run()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=389'>390</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=390'>391</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_notebook():\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36mnormal_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=344'>345</a>\u001b[0m \u001b[39m# training_dataset = prepare_data(config_dict)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=345'>346</a>\u001b[0m config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=347'>348</a>\u001b[0m train_DL(run_name, config, training_dataset)\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_DL\u001b[0;34m(run_name, config, training_dataset)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlosses/loss_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlosses/loss_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mscheduler_lr\u001b[39m\u001b[39m\"\u001b[39m, scheduler\u001b[39m.\u001b[39;49mget_lr())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m mean_dice \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmean(dices)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdice_mean_wo_bg_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_dice\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReduceLROnPlateau' object has no attribute 'get_lr'"
     ]
    }
   ],
   "source": [
    "def nan_hook(self, inp, output):\n",
    "    if not isinstance(output, tuple):\n",
    "        outputs = [output]\n",
    "    else:\n",
    "        outputs = output\n",
    "\n",
    "    for i, out in enumerate(outputs):\n",
    "        nan_mask = torch.isnan(out)\n",
    "        if nan_mask.any():\n",
    "            print(\"In\", self.__class__.__name__)\n",
    "            raise RuntimeError(f\"Found NAN in output {i} at indices: \", nan_mask.nonzero(), \"where:\", out[nan_mask.nonzero()[:, 0].unique(sorted=True)])\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu'):\n",
    "    _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    model = BlendowskiAE(in_channels=num_classes, out_channels=num_classes)\n",
    "\n",
    "    model.to(device)\n",
    "    print(f\"Param count model: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True)\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        print(f\"Loading model, optimizers and grad scalers from {_path}\")\n",
    "        model.load_state_dict(torch.load(_path.joinpath('model.pth'), map_location=device))\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scheduler.load_state_dict(torch.load(_path.joinpath('scheduler.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "    else:\n",
    "        print(f\"Generating fresh '{type(model).__name__}' model, optimizer and grad scaler.\")\n",
    "\n",
    "    for submodule in model.modules():\n",
    "        submodule.register_forward_hook(nan_hook)\n",
    "    return (model, optimizer, scheduler, scaler)\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "def inference_wrap(model, seg, use_2d, use_mind):\n",
    "    with torch.inference_mode():\n",
    "        b_seg = seg.unsqueeze(0).unsqueeze(0).float()\n",
    "        b_out = model(b_seg)[0]\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "\n",
    "\n",
    "def train_DL(run_name, config, training_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    kf = KFold(n_splits=config.num_folds)\n",
    "    # kf.get_n_splits(training_dataset.__len__(use_2d_override=False))\n",
    "    fold_iter = enumerate(kf.split(range(training_dataset.__len__(use_2d_override=False))))\n",
    "\n",
    "    if config.get('fold_override', None):\n",
    "        selected_fold = config.get('fold_override', 0)\n",
    "        fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "    elif config.only_first_fold:\n",
    "        fold_iter = list(fold_iter)[0:1]\n",
    "\n",
    "    if config.use_2d_normal_to is not None:\n",
    "        n_dims = (-2,-1)\n",
    "    else:\n",
    "        n_dims = (-3,-2,-1)\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        val_ids = training_dataset.switch_3d_identifiers(val_idxs)\n",
    "\n",
    "        print(f\"Will run validation with these 3D samples (#{len(val_ids)}):\", sorted(val_ids))\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        # val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "\n",
    "        train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "            sampler=train_subsampler, pin_memory=False, drop_last=False,\n",
    "            # collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "        )\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        if 'checkpoint_epx' in config and config['checkpoint_epx'] is not None:\n",
    "            epx_start = config['checkpoint_epx']\n",
    "        else:\n",
    "            epx_start = 0\n",
    "\n",
    "        if config.checkpoint_name:\n",
    "            # Load from checkpoint\n",
    "            _path = f\"{config.mdl_save_prefix}/{config.checkpoint_name}_fold{fold_idx}_epx{epx_start}\"\n",
    "        else:\n",
    "            _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx_start}\"\n",
    "\n",
    "        (model, optimizer, scheduler, scaler) = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=_path, device=config.device)\n",
    "\n",
    "        all_bn_counts = torch.zeros([len(training_dataset.label_tags)], device='cpu')\n",
    "\n",
    "        for bn_counts in training_dataset.bincounts_3d.values():\n",
    "            all_bn_counts += bn_counts\n",
    "\n",
    "        class_weights = 1 / (all_bn_counts).float().pow(.35)\n",
    "        class_weights /= class_weights.mean()\n",
    "\n",
    "        class_weights = class_weights.to(device=config.device)\n",
    "\n",
    "        autocast_enabled = 'cuda' in config.device\n",
    "            \n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            ### Disturb samples ###\n",
    "            training_dataset.train(use_modified=False)\n",
    "\n",
    "            epx_losses = []\n",
    "            dices = []\n",
    "            class_dices = []\n",
    "\n",
    "            # Load data\n",
    "            for batch_idx, batch in tqdm(enumerate(train_dataloader), desc=\"batch:\", total=len(train_dataloader)):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                b_hla_slc_seg = batch['hla_label_slc']\n",
    "                b_sa_slc_seg = batch['sa_label_slc']\n",
    "                b_input = torch.cat(\n",
    "                    [b_sa_slc_seg.unsqueeze(1).repeat(1,64,1,1),\n",
    "                     b_hla_slc_seg.unsqueeze(1).repeat(1,64,1,1)],\n",
    "                     dim=1\n",
    "                )\n",
    "                b_seg = batch['label']\n",
    "\n",
    "                b_input = b_input.to(device=config.device)\n",
    "                b_seg = b_seg.to(device=config.device)\n",
    "\n",
    "                b_input = F.one_hot(b_input, len(training_dataset.label_tags)).permute(0,4,1,2,3)\n",
    "                b_input = b_input.float()\n",
    "\n",
    "                ### Forward pass ###\n",
    "                with amp.autocast(enabled=autocast_enabled):\n",
    "                    assert b_input.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input image for model must be {len(n_dims)+2}D: BxCxSPATIAL but is {b_input.shape}\"\n",
    "                    for param in model.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "                    model.use_checkpointing = True\n",
    "                    logits = model(b_input)[0]\n",
    "\n",
    "                    ### Calculate loss ###\n",
    "                    assert logits.dim() == len(n_dims)+2, \\\n",
    "                        f\"Input shape for loss must be BxNUM_CLASSESxSPATIAL but is {logits.shape}\"\n",
    "                    assert b_seg.dim() == len(n_dims)+1, \\\n",
    "                        f\"Target shape for loss must be BxSPATIAL but is {b_seg.shape}\"\n",
    "\n",
    "                    ce_loss = nn.CrossEntropyLoss(class_weights)(logits, b_seg)\n",
    "\n",
    "                    if torch.any(torch.isnan(ce_loss)):\n",
    "                        raise RuntimeError(\"NaNs detetected.\")\n",
    "\n",
    "                    scaler.scale(ce_loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "\n",
    "                    epx_losses.append(ce_loss.item())\n",
    "\n",
    "                logits_for_score = logits.argmax(1)\n",
    "\n",
    "                # Calculate dice score\n",
    "                b_dice = dice3d(\n",
    "                    torch.nn.functional.one_hot(logits_for_score, len(training_dataset.label_tags)),\n",
    "                    torch.nn.functional.one_hot(b_seg, len(training_dataset.label_tags)), # Calculate dice score with original segmentation (no disturbance)\n",
    "                    one_hot_torch_style=True\n",
    "                )\n",
    "\n",
    "                dices.append(get_batch_dice_over_all(\n",
    "                    b_dice, exclude_bg=True))\n",
    "                class_dices.append(get_batch_dice_per_class(\n",
    "                    b_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                ###  Scheduler management ###\n",
    "                if config.use_scheduling:\n",
    "                    scheduler.step()\n",
    "\n",
    "                if config.debug:\n",
    "                    break\n",
    "\n",
    "            ### Logging ###\n",
    "            print(f\"### Log epoch {epx}\")\n",
    "            print(\"### Training\")\n",
    "\n",
    "            ### Log wandb data ###\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            mean_loss = torch.tensor(epx_losses).mean()\n",
    "            wandb.log({f'losses/loss_fold{fold_idx}': mean_loss}, step=global_idx)\n",
    "            print(f'losses/loss_fold{fold_idx}', f\"{mean_loss}\")\n",
    "            print(f'losses/loss_fold{fold_idx}', f\"{mean_loss}\")\n",
    "\n",
    "            mean_dice = np.nanmean(dices)\n",
    "            print(f'dice_mean_wo_bg_fold{fold_idx}', f\"{mean_dice*100:.2f}%\")\n",
    "            wandb.log({f'scores/dice_mean_wo_bg_fold{fold_idx}': mean_dice}, step=global_idx)\n",
    "\n",
    "            log_class_dices(\"scores/dice_mean_\", f\"_fold{fold_idx}\", class_dices, global_idx)\n",
    "\n",
    "            if (epx % config.save_every == 0) \\\n",
    "                or (epx+1 == config.epochs):\n",
    "                _path = f\"{config.mdl_save_prefix}/{wandb.run.name}_fold{fold_idx}_epx{epx}\"\n",
    "                save_model(\n",
    "                    Path(THIS_SCRIPT_DIR, _path),\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    scaler=scaler)\n",
    "\n",
    "                (model, optimizer, scheduler, scaler) = \\\n",
    "                    get_model(\n",
    "                        config, len(training_dataset),\n",
    "                        len(training_dataset.label_tags),\n",
    "                        THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                        _path=_path, device=config.device)\n",
    "\n",
    "            print()\n",
    "            print(\"### Validation\")\n",
    "            model.eval()\n",
    "            training_dataset.eval()\n",
    "\n",
    "            val_dices = []\n",
    "            val_class_dices = []\n",
    "\n",
    "            with amp.autocast(enabled=autocast_enabled):\n",
    "                with torch.no_grad():\n",
    "                    for val_idx in val_idxs:\n",
    "                        val_sample = training_dataset[val_idx]\n",
    "\n",
    "                        # Create batch out of single val sample\n",
    "                        b_val_hla_slc_seg = val_sample['hla_label_slc'].unsqueeze(0)\n",
    "                        b_val_sa_slc_seg = val_sample['sa_label_slc'].unsqueeze(0)\n",
    "                        b_val_input = torch.cat([\n",
    "                            b_val_sa_slc_seg.unsqueeze(1).repeat(1,64,1,1),\n",
    "                            b_val_hla_slc_seg.unsqueeze(1).repeat(1,64,1,1)\n",
    "                        ], dim=1)\n",
    "\n",
    "                        b_val_seg = val_sample['label'].unsqueeze(0)\n",
    "\n",
    "                        b_val_input = b_val_input.to(device=config.device)\n",
    "                        b_val_seg = b_val_seg.to(device=config.device)\n",
    "\n",
    "                        b_val_input = F.one_hot(b_val_input, len(training_dataset.label_tags)).permute(0,4,1,2,3)\n",
    "                        b_val_input = b_val_input.float()\n",
    "  \n",
    "                        output_val = model(b_val_input)[0]\n",
    "                        val_logits_for_score = output_val.argmax(1)\n",
    "\n",
    "                        b_val_dice = dice3d(\n",
    "                            torch.nn.functional.one_hot(val_logits_for_score, len(training_dataset.label_tags)),\n",
    "                            torch.nn.functional.one_hot(b_val_seg, len(training_dataset.label_tags)),\n",
    "                            one_hot_torch_style=True\n",
    "                        )\n",
    "\n",
    "                        # Get mean score over batch\n",
    "                        val_dices.append(get_batch_dice_over_all(\n",
    "                            b_val_dice, exclude_bg=True))\n",
    "\n",
    "                        val_class_dices.append(get_batch_dice_per_class(\n",
    "                            b_val_dice, training_dataset.label_tags, exclude_bg=True))\n",
    "\n",
    "                    mean_val_dice = np.nanmean(val_dices)\n",
    "                    print(f'val_dice_mean_wo_bg_fold{fold_idx}', f\"{mean_val_dice*100:.2f}%\")\n",
    "                    wandb.log({f'scores/val_dice_mean_wo_bg_fold{fold_idx}': mean_val_dice}, step=global_idx)\n",
    "                    log_class_dices(\"scores/val_dice_mean_\", f\"_fold{fold_idx}\", val_class_dices, global_idx)\n",
    "\n",
    "            print()\n",
    "            # End of training loop\n",
    "\n",
    "            if config.debug:\n",
    "                break\n",
    "\n",
    "        # End of fold loop\n",
    "\n",
    "\n",
    "# %%\n",
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_name'] = 'ethereal-serenity-1138'\n",
    "# config_dict['fold_override'] = 0\n",
    "# config_dict['checkpoint_epx'] = 39\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_left_atrium_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        # data_param_mode=dict(\n",
    "        #     values=[\n",
    "        #         DataParamMode.INSTANCE_PARAMS,\n",
    "        #         DataParamMode.DISABLED,\n",
    "        #     ]\n",
    "        # ),\n",
    "        use_risk_regularization=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        use_fixed_weighting=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        # fixed_weight_min_quantile=dict(\n",
    "        #     values=[0.9, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "        # ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# %%\n",
    "\n",
    "def normal_run():\n",
    "    with wandb.init(project=PROJECT_NAME, group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        # training_dataset = prepare_data(config_dict)\n",
    "        config = wandb.config\n",
    "\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        training_dataset = prepare_data(config)\n",
    "        config = wandb.config\n",
    "\n",
    "        train_DL(run_name, config, training_dataset)\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=PROJECT_NAME)\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()\n",
    "\n",
    "# %%\n",
    "if not in_notebook():\n",
    "    sys.exit(0)\n",
    "\n",
    "# %%\n",
    "# Do any postprocessing / visualization in notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0547b16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "50e8264c9cb9bb8a6cada87af39a6b7aa8c2638398580ca823279198d429a8d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
