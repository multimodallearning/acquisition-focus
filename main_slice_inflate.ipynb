{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1294502f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Recommended gpus on this machine (descending order) ###\n",
      "  ID  Card name                      Util    Mem free  Cuda             User(s)\n",
      "----  --------------------------  -------  ----------  ---------------  ---------\n",
      "   1  NVIDIA GeForce GTX 1080 Ti      0 %   11178 MiB  11.4(470.63.01)\n",
      "   2  NVIDIA GeForce GTX 1080 Ti      0 %   11178 MiB  11.4(470.63.01)\n",
      "   3  NVIDIA GeForce GTX 1080 Ti      0 %   11178 MiB  11.4(470.63.01)\n",
      "   0  NVIDIA GeForce GTX 1080 Ti  ! 100 %    4623 MiB  11.4(470.63.01)  weihsbach\n",
      "\n",
      "Will apply following mapping\n",
      "\n",
      "  ID  Card name                       torch\n",
      "----  --------------------------  --  -------\n",
      "   1  NVIDIA GeForce GTX 1080 Ti  ->  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "os.environ['MMWHS_CACHE_PATH'] = str(Path('.', '.cache'))\n",
    "\n",
    "from meidic_vtach_utils.run_on_recommended_cuda import get_cuda_environ_vars as get_vars\n",
    "os.environ.update(get_vars('select_interactively'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.cuda.amp as amp\n",
    "\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import nibabel as nib\n",
    "\n",
    "from slice_inflate.datasets.mmwhs_dataset import MMWHSDataset, load_data, extract_2d_data\n",
    "from slice_inflate.utils.common_utils import DotDict, get_script_dir, in_notebook\n",
    "from slice_inflate.utils.torch_utils import reset_determinism, ensure_dense, \\\n",
    "    get_batch_dice_over_all, get_batch_score_per_label, save_model, \\\n",
    "    reduce_label_scores_epoch, get_test_func_all_parameters_updated\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from slice_inflate.datasets.align_mmwhs import cut_slice\n",
    "from slice_inflate.utils.log_utils import get_global_idx, log_label_metrics, log_oa_metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader\n",
    "from slice_inflate.losses.dice_loss import DC_and_CE_loss\n",
    "\n",
    "from mdl_seg_class.metrics import dice3d, hausdorff3d\n",
    "import numpy as np\n",
    "\n",
    "from slice_inflate.models.generic_UNet_opt_skip_connections import Generic_UNet\n",
    "import dill\n",
    "\n",
    "import einops as eo\n",
    "\n",
    "THIS_SCRIPT_DIR = get_script_dir()\n",
    "\n",
    "PROJECT_NAME = \"slice_inflate\"\n",
    "\n",
    "training_dataset, test_dataset = None, None\n",
    "test_all_parameters_updated = get_test_func_all_parameters_updated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3fce271",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(Path(THIS_SCRIPT_DIR, 'config_dict.json'), 'r') as f:\n",
    "    config_dict = DotDict(json.load(f))\n",
    "\n",
    "def prepare_data(config):\n",
    "    training_dataset = MMWHSDataset(\n",
    "        config.data_base_path,\n",
    "        state=config.state,\n",
    "        load_func=load_data,\n",
    "        extract_slice_func=extract_2d_data,\n",
    "        modality=config.modality,\n",
    "        do_align_global=True,\n",
    "        do_resample=False, # Prior to cropping, resample image?\n",
    "        crop_3d_region=None, # Crop or pad the images to these dimensions\n",
    "        fov_mm=config.fov_mm,\n",
    "        fov_vox=config.fov_vox,\n",
    "        crop_around_3d_label_center=config.crop_around_3d_label_center,\n",
    "        pre_interpolation_factor=1., # When getting the data, resize the data by this factor\n",
    "        ensure_labeled_pairs=True, # Only use fully labelled images (segmentation label available)\n",
    "        use_2d_normal_to=config.use_2d_normal_to, # Use 2D slices cut normal to D,H,>W< dimensions\n",
    "        crop_around_2d_label_center=config.crop_around_2d_label_center,\n",
    "        max_load_3d_num=config.max_load_3d_num,\n",
    "\n",
    "        augment_angle_std=5,\n",
    "\n",
    "        device=config.device,\n",
    "        debug=config.debug\n",
    "    )\n",
    "\n",
    "    return training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14d2d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMWHS training images and labels... (['mr'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20 images, 20 labels: 100%|██████████| 40/40 [00:26<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postprocessing 3D volumes\n",
      "Removed 0 3D images in postprocessing\n",
      "Equal image and label numbers: True (20)\n",
      "Data import finished.\n",
      "Dataloader will yield 3D samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_test_once_only = not (config_dict.test_only_and_output_to in [\"\", None])\n",
    "\n",
    "if training_dataset is None:\n",
    "    train_config = DotDict(config_dict.copy())\n",
    "    if run_test_once_only:\n",
    "        train_config['state'] = 'empty'\n",
    "    training_dataset = prepare_data(train_config)\n",
    "\n",
    "if test_dataset is None:\n",
    "    test_config = DotDict(config_dict.copy())\n",
    "    test_config['state'] = 'test'\n",
    "    test_dataset = prepare_data(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "447c2722",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset.train(augment=True)\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 5\n",
    "    print(\"do_augment\", training_dataset.do_augment)\n",
    "    for sample in [training_dataset[idx] for idx in range(20)]:\n",
    "        fig = plt.figure(figsize=(16., 4.))\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "            nrows_ncols=(1, 6),  # creates 2x2 grid of axes\n",
    "            axes_pad=0.0,  # pad between axes in inch.\n",
    "        )\n",
    "\n",
    "        show_row = [\n",
    "            cut_slice(sample['image']),\n",
    "            cut_slice(sample['label']),\n",
    "\n",
    "            sample['sa_image_slc'],\n",
    "            sample['sa_label_slc'],\n",
    "\n",
    "            sample['hla_image_slc'],\n",
    "            sample['hla_label_slc'],\n",
    "        ]\n",
    "\n",
    "        for ax, im in zip(grid, show_row):\n",
    "            ax.imshow(im, cmap='gray', interpolation='none')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49c0b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset.train()\n",
    "\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 5\n",
    "    print(\"do_augment\", training_dataset.do_augment)\n",
    "    for sample_idx in range(20):\n",
    "        lbl, sa_label, hla_label = torch.zeros(128,128), torch.zeros(128,128), torch.zeros(128,128)\n",
    "        for augment_idx in range(15):\n",
    "            sample = training_dataset[sample_idx]\n",
    "            nib.save(nib.Nifti1Image(sample['label'].cpu().numpy(), affine=torch.eye(4).numpy()), f'out{sample_idx}.nii.gz')\n",
    "            lbl += cut_slice(sample['label']).cpu()\n",
    "            sa_label += sample['sa_label_slc'].cpu()\n",
    "            hla_label += sample['hla_label_slc'].cpu()\n",
    "\n",
    "        fig = plt.figure(figsize=(16., 4.))\n",
    "        grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "            nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "            axes_pad=0.0,  # pad between axes in inch.\n",
    "        )\n",
    "\n",
    "        show_row = [\n",
    "            lbl, sa_label, hla_label\n",
    "        ]\n",
    "\n",
    "        for ax, im in zip(grid, show_row):\n",
    "            ax.imshow(im, cmap='magma', interpolation='none')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9d0a5a8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    training_dataset.train(augment=False)\n",
    "    training_dataset.self_attributes['augment_angle_std'] = 2\n",
    "    print(training_dataset.do_augment)\n",
    "\n",
    "    lbl, sa_label, hla_label = torch.zeros(128,128), torch.zeros(128,128), torch.zeros(128,128)\n",
    "    for tr_idx in range(len(training_dataset)):\n",
    "        sample = training_dataset[tr_idx]\n",
    "\n",
    "        lbl += cut_slice(sample['label']).cpu()\n",
    "        sa_label += sample['sa_label_slc'].cpu()\n",
    "        hla_label += sample['hla_label_slc'].cpu()\n",
    "\n",
    "    fig = plt.figure(figsize=(16., 4.))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "        nrows_ncols=(1, 3),  # creates 2x2 grid of axes\n",
    "        axes_pad=0.0,  # pad between axes in inch.\n",
    "    )\n",
    "\n",
    "    show_row = [\n",
    "        lbl, sa_label, hla_label\n",
    "    ]\n",
    "\n",
    "    for ax, im in zip(grid, show_row):\n",
    "        ax.imshow(im, cmap='magma', interpolation='none')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0547b16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendowskiAE(torch.nn.Module):\n",
    "\n",
    "    class ConvBlock(torch.nn.Module):\n",
    "        def __init__(self, in_channels: int, out_channels_list: list, strides_list: list, kernels_list:list=None, paddings_list:list=None):\n",
    "            super().__init__()\n",
    "\n",
    "            ops = []\n",
    "            in_channels = [in_channels] + out_channels_list[:-1]\n",
    "            if kernels_list is None:\n",
    "                kernels_list = [3] * len(out_channels_list)\n",
    "            if paddings_list is None:\n",
    "                paddings_list = [1] * len(out_channels_list)\n",
    "\n",
    "            for op_idx in range(len(out_channels_list)):\n",
    "                ops.append(torch.nn.Conv3d(\n",
    "                    in_channels[op_idx],\n",
    "                    out_channels_list[op_idx],\n",
    "                    kernel_size=kernels_list[op_idx],\n",
    "                    stride=strides_list[op_idx],\n",
    "                    padding=paddings_list[op_idx]\n",
    "                ))\n",
    "                ops.append(torch.nn.BatchNorm3d(out_channels_list[op_idx]))\n",
    "                ops.append(torch.nn.LeakyReLU())\n",
    "\n",
    "            self.block = torch.nn.Sequential(*ops)\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, decoder_in_channels=2, debug_mode=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.debug_mode = debug_mode\n",
    "\n",
    "        self.first_layer_encoder = self.ConvBlock(in_channels, out_channels_list=[8], strides_list=[1])\n",
    "        self.first_layer_decoder = self.ConvBlock(8, out_channels_list=[8,out_channels], strides_list=[1,1])\n",
    "\n",
    "        self.second_layer_encoder = self.ConvBlock(8, out_channels_list=[20,20,20], strides_list=[2,1,1])\n",
    "        self.second_layer_decoder = self.ConvBlock(20, out_channels_list=[8], strides_list=[1])\n",
    "\n",
    "        self.third_layer_encoder = self.ConvBlock(20, out_channels_list=[40,40,40], strides_list=[2,1,1])\n",
    "        self.third_layer_decoder = self.ConvBlock(40, out_channels_list=[20], strides_list=[1])\n",
    "\n",
    "        self.fourth_layer_encoder = self.ConvBlock(40, out_channels_list=[60,60,60], strides_list=[2,1,1])\n",
    "        self.fourth_layer_decoder = self.ConvBlock(decoder_in_channels, out_channels_list=[40], strides_list=[1])\n",
    "\n",
    "        self.deepest_layer = torch.nn.Sequential(\n",
    "            self.ConvBlock(60, out_channels_list=[60,40,20], strides_list=[2,1,1]),\n",
    "            torch.nn.Conv3d(20, 2, kernel_size=1, stride=1, padding=0)\n",
    "        )\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            self.first_layer_encoder,\n",
    "            self.second_layer_encoder,\n",
    "            self.third_layer_encoder,\n",
    "            self.fourth_layer_encoder,\n",
    "        )\n",
    "\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.fourth_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.third_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.second_layer_decoder,\n",
    "            torch.nn.Upsample(scale_factor=2),\n",
    "            self.first_layer_decoder,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.deepest_layer(h)\n",
    "        # h = debug_forward_pass(self.encoder, x, STEP_MODE=False)\n",
    "        # h = debug_forward_pass(self.deepest_layer, h, STEP_MODE=False)\n",
    "        return h\n",
    "\n",
    "    def decode(self, z):\n",
    "        if self.debug_mode:\n",
    "            return debug_forward_pass(self.decoder, z, STEP_MODE=False)\n",
    "        else:\n",
    "            return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.nn.functional.instance_norm(x)\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z), z\n",
    "\n",
    "\n",
    "\n",
    "class BlendowskiVAE(BlendowskiAE):\n",
    "    def __init__(self, std_max=10.0, epoch=0, epoch_reach_std_max=250, *args, **kwargs):\n",
    "        kwargs['decoder_in_channels'] = 1\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.deepest_layer_upstream = self.ConvBlock(60, out_channels_list=[60,40,20], strides_list=[2,1,1])\n",
    "        self.deepest_layer_downstream = nn.ModuleList([\n",
    "            torch.nn.Conv3d(20, 1, kernel_size=1, stride=1, padding=0),\n",
    "            torch.nn.Conv3d(20, 1, kernel_size=1, stride=1, padding=0)\n",
    "        ])\n",
    "\n",
    "        self.log_var_scale = nn.Parameter(torch.Tensor([0.0]))\n",
    "        self.epoch = epoch\n",
    "        self.epoch_reach_std_max = epoch_reach_std_max\n",
    "        self.std_max = std_max\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_std_max(self):\n",
    "        SIGMOID_XMIN, SIGMOID_XMAX = -8.0, 8.0\n",
    "        s_x = (SIGMOID_XMAX-SIGMOID_XMIN) / (self.epoch_reach_std_max - 0) * self.epoch + SIGMOID_XMIN\n",
    "        std_max = torch.sigmoid(torch.tensor(s_x)) * self.std_max\n",
    "        return std_max\n",
    "\n",
    "    def sample_z(self, mean, std):\n",
    "        q = torch.distributions.Normal(mean, std)\n",
    "        return q.rsample() # Caution, dont use torch.normal(mean=mean, std=std). Gradients are not backpropagated\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.deepest_layer_upstream(h)\n",
    "        mean = self.deepest_layer_downstream[0](h)\n",
    "        log_var = self.deepest_layer_downstream[1](h)\n",
    "        return mean, log_var\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        std = torch.exp(log_var/2)\n",
    "        std = std.clamp(min=1e-10, max=self.get_std_max())\n",
    "        z = self.sample_z(mean=mean, std=std)\n",
    "        return self.decode(z), (z, mean, std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a063e2be",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# x = torch.zeros(1,8,128,128,128)\n",
    "# bae = BlendowskiAE(in_channels=8, out_channels=8)\n",
    "\n",
    "# y, z = bae(x)\n",
    "\n",
    "# print(\"BAE\")\n",
    "# print(\"x\", x.shape)\n",
    "# print(\"z\", z.shape)\n",
    "# print(\"y\", y.shape)\n",
    "# print()\n",
    "\n",
    "# bvae = BlendowskiVAE(in_channels=8, out_channels=8)\n",
    "\n",
    "# y, z = bvae(x)\n",
    "\n",
    "# print(\"BVAE\")\n",
    "# print(\"x\", x.shape)\n",
    "# print(\"z\", z.shape)\n",
    "# print(\"y\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de04e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BlendowskiVAE(in_channels=6, out_channels=6)\n",
    "# model.cuda()\n",
    "# with torch.no_grad():\n",
    "#     smp = torch.nn.functional.one_hot(training_dataset[1]['label'], 6).unsqueeze(0).permute([0,4,1,2,3]).float().cuda()\n",
    "# y, _ = model(smp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49157799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nan_hook(self, inp, output):\n",
    "#     if not isinstance(output, tuple):\n",
    "#         outputs = [output]\n",
    "#     else:\n",
    "#         outputs = output\n",
    "\n",
    "#     for i, out in enumerate(outputs):\n",
    "#         nan_mask = torch.isnan(out)\n",
    "#         if nan_mask.any():\n",
    "#             print(\"In\", self.__class__.__name__)\n",
    "#             raise RuntimeError(f\"Found NAN in output {i} at indices: \", nan_mask.nonzero(), \"where:\", out[nan_mask.nonzero()[:, 0].unique(sorted=True)])\n",
    "\n",
    "def get_model(config, dataset_len, num_classes, THIS_SCRIPT_DIR, _path=None, device='cpu', load_model_only=False, encoder_training_only=False):\n",
    "    if not _path is None:\n",
    "        _path = Path(THIS_SCRIPT_DIR).joinpath(_path).resolve()\n",
    "\n",
    "    if config.model_type == 'vae':\n",
    "        model = BlendowskiVAE(std_max=10.0, epoch=0, epoch_reach_std_max=250,\n",
    "            in_channels=num_classes, out_channels=num_classes)\n",
    "\n",
    "    elif config.model_type == 'ae':\n",
    "        model = BlendowskiAE(in_channels=num_classes, out_channels=num_classes)\n",
    "    elif 'unet' in config.model_type:\n",
    "        init_dict_path = Path(THIS_SCRIPT_DIR, \"./slice_inflate/models/nnunet_init_dict_128_128_128.pkl\")\n",
    "        with open(init_dict_path, 'rb') as f:\n",
    "            init_dict = dill.load(f)\n",
    "        init_dict['num_classes'] = num_classes\n",
    "        init_dict['deep_supervision'] = False\n",
    "        init_dict['final_nonlin'] = torch.nn.Identity()\n",
    "        use_skip_connections = True if not 'wo-skip' in config.model_type else False\n",
    "        nnunet_model = Generic_UNet(**init_dict, use_skip_connections=use_skip_connections, use_onehot_input=True)\n",
    "\n",
    "        seg_outputs = list(filter(lambda elem: 'seg_outputs' in elem[0], nnunet_model.named_parameters()))\n",
    "        # Disable gradients of non-used deep supervision\n",
    "        for so_idx in range(len(seg_outputs)-1):\n",
    "            seg_outputs[so_idx][1].requires_grad = False\n",
    "        class InterfaceModel(torch.nn.Module):\n",
    "            def __init__(self, nnunet_model):\n",
    "                super().__init__()\n",
    "                self.nnunet_model = nnunet_model\n",
    "\n",
    "            def forward(self, x):\n",
    "                y_hat = self.nnunet_model(x)\n",
    "                if isinstance(y_hat, tuple):\n",
    "                    return y_hat[0], None\n",
    "                else:\n",
    "                    return y_hat, None\n",
    "\n",
    "        model = InterfaceModel(nnunet_model)\n",
    "\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if _path and _path.is_dir():\n",
    "        model_dict = torch.load(_path.joinpath('model.pth'), map_location=device)\n",
    "        epx = model_dict.get('metadata', {}).get('epx', 0)\n",
    "        print(f\"Loading model from {_path}\")\n",
    "        print(model.load_state_dict(model_dict, strict=False))\n",
    "    else:\n",
    "        print(f\"Generating fresh '{type(model).__name__}' model.\")\n",
    "        epx = 0\n",
    "\n",
    "    if encoder_training_only:\n",
    "        decoder_modules = filter(lambda elem: 'decoder' in elem[0], model.named_modules())\n",
    "        for nmod in decoder_modules:\n",
    "            for param in nmod[1].parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    print(f\"Trainable param count model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    print(f\"Non-trainable param count model: {sum(p.numel() for p in model.parameters() if not p.requires_grad)}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.lr)\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=20, threshold=0.01, threshold_mode='rel')\n",
    "\n",
    "    if _path and _path.is_dir() and not load_model_only:\n",
    "        print(f\"Loading optimizer, scheduler, scaler from {_path}\")\n",
    "        optimizer.load_state_dict(torch.load(_path.joinpath('optimizer.pth'), map_location=device))\n",
    "        scheduler.load_state_dict(torch.load(_path.joinpath('scheduler.pth'), map_location=device))\n",
    "        scaler.load_state_dict(torch.load(_path.joinpath('scaler.pth'), map_location=device))\n",
    "\n",
    "    else:\n",
    "        print(f\"Generating fresh optimizer, scheduler, scaler.\")\n",
    "\n",
    "    optimizer.add_param_group(dict(params=training_dataset.sa_atm.parameters()))\n",
    "    optimizer.add_param_group(dict(params=training_dataset.hla_atm.parameters()))\n",
    "\n",
    "    # for submodule in model.modules():\n",
    "    #     submodule.register_forward_hook(nan_hook)\n",
    "\n",
    "    return (model, optimizer, scheduler, scaler), epx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40a8e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input(batch, config, num_classes):\n",
    "    W_TARGET_LEN = 128\n",
    "    b_hla_slc_seg = batch['hla_label_slc']\n",
    "    b_sa_slc_seg = batch['sa_label_slc']\n",
    "    b_input = torch.stack([b_hla_slc_seg, b_sa_slc_seg], dim=2)\n",
    "    b_input = torch.cat([b_input] * int(W_TARGET_LEN/b_input.shape[2]), dim=2) # Stack data hla/sa next to each other\n",
    "\n",
    "    b_seg = batch['label']\n",
    "\n",
    "    b_input = b_input.to(device=config.device)\n",
    "    b_seg = b_seg.to(device=config.device)\n",
    "\n",
    "    return b_input, b_seg\n",
    "\n",
    "def inference_wrap(model, seg):\n",
    "    with torch.inference_mode():\n",
    "        b_seg = seg.unsqueeze(0).unsqueeze(0).float()\n",
    "        b_out = model(b_seg)[0]\n",
    "        b_out = b_out.argmax(1)\n",
    "        return b_out\n",
    "\n",
    "\n",
    "\n",
    "def gaussian_likelihood(y_hat, log_var_scale, y_target):\n",
    "    B,C,*_ = y_hat.shape\n",
    "    scale = torch.exp(log_var_scale/2)\n",
    "    dist = torch.distributions.Normal(y_hat, scale)\n",
    "\n",
    "    # measure prob of seeing image under p(x|z)\n",
    "    log_pxz = dist.log_prob(y_target)\n",
    "\n",
    "    # GLH\n",
    "    return log_pxz\n",
    "\n",
    "\n",
    "\n",
    "def kl_divergence(z, mean, std):\n",
    "    # See https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "    B,*_ = z.shape\n",
    "    p = torch.distributions.Normal(torch.zeros_like(mean), torch.ones_like(std))\n",
    "    q = torch.distributions.Normal(mean, std)\n",
    "\n",
    "    log_qzx = q.log_prob(z)\n",
    "    log_pz = p.log_prob(z)\n",
    "\n",
    "    # KL divergence\n",
    "    kl = (log_qzx - log_pz)\n",
    "\n",
    "    # Reduce spatial dimensions\n",
    "    return kl.reshape(B,-1)\n",
    "\n",
    "\n",
    "\n",
    "def get_ae_loss_value(y_hat, y_target, class_weights):\n",
    "    return DC_and_CE_loss({}, {})(y_hat, y_target.argmax(1, keepdim=True))\n",
    "\n",
    "\n",
    "def get_vae_loss_value(y_hat, y_target, z, mean, std, class_weights, model):\n",
    "    recon_loss = get_ae_loss_value(y_hat, y_target, class_weights)#torch.nn.MSELoss()(y_hat, y_target)#gaussian_likelihood(y_hat, model.log_var_scale, y_target.float())\n",
    "    # recon_loss = eo.reduce(recon_loss, 'B C spatial -> B ()', 'mean')\n",
    "    kl = kl_divergence(z, mean, std)\n",
    "\n",
    "    elbo = (0.1*kl + recon_loss).mean()\n",
    "\n",
    "    return elbo\n",
    "\n",
    "def model_step(config, model, b_input, b_target, label_tags, class_weights, io_normalisation_values, autocast_enabled=False):\n",
    "    # b_input = b_input-io_normalisation_values['input_mean'].to(b_input.device)\n",
    "    # b_input = b_input/io_normalisation_values['input_std'].to(b_input.device)\n",
    "\n",
    "    ### Forward pass ###\n",
    "    with amp.autocast(enabled=autocast_enabled):\n",
    "        assert b_input.dim() == 5, \\\n",
    "            f\"Input image for model must be {5}D: BxCxSPATIAL but is {b_input.shape}\"\n",
    "\n",
    "        if config.model_type == 'vae':\n",
    "            y_hat, (z, mean, std) = model(b_input)\n",
    "        elif config.model_type in ['ae', 'unet', 'unet-wo-skip']:\n",
    "            y_hat, _ = model(b_input)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        # Reverse normalisation to outputs\n",
    "        # y_hat = y_hat*io_normalisation_values['target_std'].to(b_input.device)\n",
    "        # y_hat = y_hat+io_normalisation_values['target_mean'].to(b_input.device)\n",
    "\n",
    "        ### Calculate loss ###\n",
    "        assert y_hat.dim() == 5, \\\n",
    "            f\"Input shape for loss must be {5}D: BxNUM_CLASSESxSPATIAL but is {y_hat.shape}\"\n",
    "        assert b_target.dim() == 5, \\\n",
    "            f\"Target shape for loss must be {5}D: BxNUM_CLASSESxSPATIAL but is {b_target.shape}\"\n",
    "\n",
    "        if \"vae\" in type(model).__name__.lower():\n",
    "            loss = get_vae_loss_value(y_hat, b_target.float(), z, mean, std, class_weights, model)\n",
    "        else:\n",
    "            loss = get_ae_loss_value(y_hat, b_target.float(), class_weights)\n",
    "\n",
    "    return y_hat, loss\n",
    "\n",
    "\n",
    "\n",
    "def epoch_iter(epx, global_idx, config, model, dataset, dataloader, class_weights, fold_postfix, phase='train',\n",
    "    autocast_enabled=False, optimizer=None, scaler=None, store_net_output_to=None):\n",
    "    PHASES = ['train', 'val', 'test']\n",
    "    assert phase in ['train', 'val', 'test'], f\"phase must be one of {PHASES}\"\n",
    "\n",
    "    epx_losses = []\n",
    "    label_scores_epoch = {}\n",
    "    seg_metrics_nanmean = {}\n",
    "    seg_metrics_std = {}\n",
    "    seg_metrics_nanmean_oa = {}\n",
    "    seg_metrics_std_oa = {}\n",
    "\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "        dataset.train(use_modified=False)\n",
    "    else:\n",
    "        model.eval()\n",
    "        dataset.eval()\n",
    "\n",
    "    if isinstance(model, BlendowskiVAE):\n",
    "        model.set_epoch(epx)\n",
    "\n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader), desc=phase, total=len(dataloader)):\n",
    "        b_input, b_seg = get_model_input(batch, config, len(dataset.label_tags))\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            y_hat, loss = model_step(config, model, b_input, b_seg, dataset.label_tags, class_weights, dataset.io_normalisation_values, autocast_enabled)\n",
    "            scaler.scale(loss).backward()\n",
    "            # test_all_parameters_updated(model)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                y_hat, loss = model_step(config, model, b_input, b_seg, dataset.label_tags, class_weights, dataset.io_normalisation_values, autocast_enabled)\n",
    "\n",
    "        epx_losses.append(loss.item())\n",
    "\n",
    "        pred_seg = y_hat.argmax(1)\n",
    "\n",
    "        # Taken from nibabel nifti1.py\n",
    "        RZS = batch['sa_affine'][0][:3,:3].detach().numpy()\n",
    "        nifti_zooms = np.sqrt(np.sum(RZS * RZS, axis=0))\n",
    "\n",
    "        # Calculate fast dice score\n",
    "        b_dice = dice3d(\n",
    "            eo.rearrange(torch.nn.functional.one_hot(pred_seg, len(training_dataset.label_tags)), 'b d h w oh -> b oh d h w'),\n",
    "            b_seg,\n",
    "            one_hot_torch_style=False\n",
    "        )\n",
    "        label_scores_epoch = get_batch_score_per_label(label_scores_epoch, 'dice',\n",
    "            b_dice, training_dataset.label_tags, exclude_bg=True)\n",
    "\n",
    "        if epx % 20 == 0 and epx > 0:\n",
    "            b_hd = hausdorff3d(b_input, b_seg, spacing_mm=tuple(nifti_zooms), percent=100)\n",
    "            label_scores_epoch = get_batch_score_per_label(label_scores_epoch, 'hd',\n",
    "                b_hd, training_dataset.label_tags, exclude_bg=True)\n",
    "\n",
    "            b_hd95 = hausdorff3d(b_input, b_seg, spacing_mm=tuple(nifti_zooms), percent=95)\n",
    "            label_scores_epoch = get_batch_score_per_label(label_scores_epoch, 'hd95',\n",
    "                b_hd95, training_dataset.label_tags, exclude_bg=True)\n",
    "\n",
    "        if store_net_output_to not in [\"\", None]:\n",
    "            store_path = Path(store_net_output_to, f\"output_batch{batch_idx}.pth\")\n",
    "            store_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "            torch.save(dict(batch=batch, input=b_input, output=y_hat, target=b_seg), store_path)\n",
    "\n",
    "        if config.debug: break\n",
    "\n",
    "    (seg_metrics_nanmean,\n",
    "     seg_metrics_std,\n",
    "     seg_metrics_nanmean_oa,\n",
    "     seg_metrics_std_oa) = reduce_label_scores_epoch(label_scores_epoch)\n",
    "\n",
    "    loss_mean = torch.tensor(epx_losses).mean()\n",
    "    ### Logging ###\n",
    "    print(f\"### {phase.upper()}\")\n",
    "\n",
    "    ### Log wandb data ###\n",
    "    log_id = f'losses/{phase}_loss{fold_postfix}'\n",
    "    log_val = loss_mean\n",
    "    wandb.log({log_id: log_val}, step=global_idx)\n",
    "    print(f'losses/{phase}_loss{fold_postfix}', log_val)\n",
    "\n",
    "    log_label_metrics(f\"scores/{phase}_mean\", fold_postfix, seg_metrics_nanmean, global_idx,\n",
    "        logger_selected_metrics=('dice', 'hd', 'hd95'), print_selected_metrics=('dice'))\n",
    "\n",
    "    log_label_metrics(f\"scores/{phase}_std\", fold_postfix, seg_metrics_std, global_idx,\n",
    "        logger_selected_metrics=('dice', 'hd', 'hd95'), print_selected_metrics=())\n",
    "\n",
    "    log_oa_metrics(f\"scores/{phase}_mean_oa_exclude_bg\", fold_postfix, seg_metrics_nanmean_oa, global_idx,\n",
    "        logger_selected_metrics=('dice', 'hd', 'hd95'), print_selected_metrics=('dice', 'hd', 'hd95'))\n",
    "\n",
    "    log_oa_metrics(f\"scores/{phase}_std_oa_exclude_bg\", fold_postfix, seg_metrics_std_oa, global_idx,\n",
    "        logger_selected_metrics=('dice', 'hd', 'hd95'), print_selected_metrics=())\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    return loss_mean\n",
    "\n",
    "\n",
    "\n",
    "def run_dl(run_name, config, training_dataset, test_dataset):\n",
    "    reset_determinism()\n",
    "\n",
    "    # Configure folds\n",
    "    if config.num_folds < 1:\n",
    "        train_idxs = range(training_dataset.__len__(use_2d_override=False))\n",
    "        val_idxs = []\n",
    "        fold_idx = -1\n",
    "        fold_iter = ([fold_idx, (train_idxs, val_idxs)],)\n",
    "\n",
    "    else:\n",
    "        kf = KFold(n_splits=config.num_folds)\n",
    "        fold_iter = enumerate(kf.split(range(training_dataset.__len__(use_2d_override=False))))\n",
    "\n",
    "        if config.get('fold_override', None):\n",
    "            selected_fold = config.get('fold_override', 0)\n",
    "            fold_iter = list(fold_iter)[selected_fold:selected_fold+1]\n",
    "\n",
    "    fold_means_no_bg = []\n",
    "\n",
    "    for fold_idx, (train_idxs, val_idxs) in fold_iter:\n",
    "        fold_postfix = f'_fold{fold_idx}' if fold_idx != -1 else \"\"\n",
    "\n",
    "        best_quality_metric = 1.e16\n",
    "        train_idxs = torch.tensor(train_idxs)\n",
    "        val_idxs = torch.tensor(val_idxs)\n",
    "        val_ids = training_dataset.switch_3d_identifiers(val_idxs)\n",
    "\n",
    "        print(f\"Will run validation with these 3D samples (#{len(val_ids)}):\", sorted(val_ids))\n",
    "\n",
    "        ### Add train sampler and dataloaders ##\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idxs)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idxs)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(range(len(test_dataset)))\n",
    "\n",
    "        if not run_test_once_only:\n",
    "            train_dataloader = DataLoader(training_dataset, batch_size=config.batch_size,\n",
    "                sampler=train_subsampler, pin_memory=False, drop_last=False,\n",
    "                collate_fn=training_dataset.get_efficient_augmentation_collate_fn()\n",
    "            )\n",
    "            training_dataset.set_augment_at_collate(True)\n",
    "\n",
    "            val_dataloader = DataLoader(training_dataset, batch_size=config.val_batch_size,\n",
    "                sampler=val_subsampler, pin_memory=False, drop_last=False\n",
    "            )\n",
    "\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=config.val_batch_size,\n",
    "            sampler=test_subsampler, pin_memory=False, drop_last=False\n",
    "        )\n",
    "\n",
    "        # Load from checkpoint, if any\n",
    "        chk_path = config.checkpoint_path if 'checkpoint_path' in config else None\n",
    "\n",
    "        ### Get model, data parameters, optimizers for model and data parameters, as well as grad scaler ###\n",
    "        (model, optimizer, scheduler, scaler), epx_start = get_model(config, len(training_dataset), len(training_dataset.label_tags),\n",
    "            THIS_SCRIPT_DIR=THIS_SCRIPT_DIR, _path=chk_path, device=config.device, load_model_only=False, encoder_training_only=config.encoder_training_only)\n",
    "\n",
    "        all_bn_counts = torch.zeros([len(training_dataset.label_tags)], device='cpu')\n",
    "\n",
    "        # for bn_counts in training_dataset.bincounts_3d.values():\n",
    "        #     all_bn_counts += bn_counts\n",
    "\n",
    "        # class_weights = 1 / (all_bn_counts).float().pow(.35)\n",
    "        # class_weights /= class_weights.mean()\n",
    "\n",
    "        # class_weights = class_weights.to(device=config.device)\n",
    "        class_weights = None\n",
    "\n",
    "        autocast_enabled = 'cuda' in config.device\n",
    "\n",
    "        for epx in range(epx_start, config.epochs):\n",
    "            global_idx = get_global_idx(fold_idx, epx, config.epochs)\n",
    "            # Log the epoch idx per fold - so we can recover the diagram by setting\n",
    "            # ref_epoch_idx as x-axis in wandb interface\n",
    "            print(f\"### Log epoch {epx}\")\n",
    "            wandb.log({\"ref_epoch_idx\": epx}, step=global_idx)\n",
    "\n",
    "            if not run_test_once_only:\n",
    "                train_loss = epoch_iter(epx, global_idx, config, model, training_dataset, train_dataloader, class_weights, fold_postfix,\n",
    "                    phase='train', autocast_enabled=autocast_enabled, optimizer=optimizer, scaler=scaler, store_net_output_to=None)\n",
    "\n",
    "                val_loss = epoch_iter(epx, global_idx, config, model, training_dataset, val_dataloader, class_weights, fold_postfix,\n",
    "                    phase='val', autocast_enabled=autocast_enabled, optimizer=None, scaler=None, store_net_output_to=None)\n",
    "\n",
    "            quality_metric = test_loss = epoch_iter(epx, global_idx, config, model, test_dataset, test_dataloader, class_weights, fold_postfix,\n",
    "                phase='test', autocast_enabled=autocast_enabled, optimizer=None, scaler=None, store_net_output_to=config.test_only_and_output_to)\n",
    "\n",
    "\n",
    "            if run_test_once_only:\n",
    "                break\n",
    "\n",
    "            ###  Scheduler management ###\n",
    "            if config.use_scheduling:\n",
    "                scheduler.step(quality_metric)\n",
    "\n",
    "            wandb.log({f'training/scheduler_lr': scheduler.optimizer.param_groups[0]['lr']}, step=global_idx)\n",
    "            print()\n",
    "\n",
    "            # Save model\n",
    "            if config.save_every is None:\n",
    "                pass\n",
    "\n",
    "            elif config.save_every == 'best':\n",
    "                if quality_metric < best_quality_metric:\n",
    "                    best_quality_metric = quality_metric\n",
    "                    save_path = f\"{config.mdl_save_prefix}/{wandb.run.name}{fold_postfix}_best\"\n",
    "                    save_model(\n",
    "                        Path(THIS_SCRIPT_DIR, save_path),\n",
    "                        epx=epx,\n",
    "                        loss=train_loss,\n",
    "                        model=model,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        scaler=scaler)\n",
    "\n",
    "            elif (epx % config.save_every == 0) or (epx+1 == config.epochs):\n",
    "                save_path = f\"{config.mdl_save_prefix}/{wandb.run.name}{fold_postfix}_epx{epx}\"\n",
    "                save_model(\n",
    "                    Path(THIS_SCRIPT_DIR, save_path),\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    scheduler=scheduler,\n",
    "                    scaler=scaler)\n",
    "\n",
    "                # (model, optimizer, scheduler, scaler) = \\\n",
    "                #     get_model(\n",
    "                #         config, len(training_dataset),\n",
    "                #         len(training_dataset.label_tags),\n",
    "                #         THIS_SCRIPT_DIR=THIS_SCRIPT_DIR,\n",
    "                #         _path=_path, device=config.device)\n",
    "\n",
    "            # End of epoch loop\n",
    "\n",
    "            if config.debug or run_test_once_only:\n",
    "                break\n",
    "\n",
    "        # End of fold loop\n",
    "        if config.debug or run_test_once_only:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38e3142",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# training_dataset.eval()\n",
    "# eval_dataloader = DataLoader(training_dataset, batch_size=20,  pin_memory=False, drop_last=False)\n",
    "\n",
    "# for large_batch in eval_dataloader:\n",
    "#     large_b_input = get_model_input(large_batch, config_dict, num_classes=len(training_dataset.label_tags))\n",
    "\n",
    "# input_mean, input_std = large_b_input[0].float().mean((0,-3,-2,-1), keepdim=True).cpu(), large_b_input[0].float().std((0,-3,-2,-1), keepdim=True).cpu()\n",
    "# target_mean, target_std = large_b_input[1].float().mean((0,-3,-2,-1), keepdim=True).cpu(), large_b_input[1].float().std((0,-3,-2,-1), keepdim=True).cpu()\n",
    "\n",
    "# print(input_mean.shape, input_std.shape)\n",
    "# print(target_mean.shape, target_std.shape)\n",
    "\n",
    "# torch.save(dict(input_mean=input_mean, input_std=input_std, target_mean=target_mean, target_std=target_std), \"io_normalisation_values.pth\")\n",
    "# sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df6996f5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Config overrides\n",
    "# config_dict['wandb_mode'] = 'disabled'\n",
    "# config_dict['debug'] = True\n",
    "# Model loading\n",
    "# config_dict['checkpoint_path'] = 'ethereal-serenity-1138'\n",
    "# config_dict['fold_override'] = 0\n",
    "\n",
    "# Define sweep override dict\n",
    "sweep_config_dict = dict(\n",
    "    method='grid',\n",
    "    metric=dict(goal='maximize', name='scores/val_dice_mean_left_atrium_fold0'),\n",
    "    parameters=dict(\n",
    "        # disturbance_mode=dict(\n",
    "        #     values=[\n",
    "        #        'LabelDisturbanceMode.AFFINE',\n",
    "        #     ]\n",
    "        # ),\n",
    "        # disturbance_strength=dict(\n",
    "        #     values=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0]\n",
    "        # ),\n",
    "        # disturbed_percentage=dict(\n",
    "        #     values=[0.3, 0.6]\n",
    "        # ),\n",
    "        # data_param_mode=dict(\n",
    "        #     values=[\n",
    "        #         DataParamMode.INSTANCE_PARAMS,\n",
    "        #         DataParamMode.DISABLED,\n",
    "        #     ]\n",
    "        # ),\n",
    "        use_risk_regularization=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        use_fixed_weighting=dict(\n",
    "            values=[False, True]\n",
    "        ),\n",
    "        # fixed_weight_min_quantile=dict(\n",
    "        #     values=[0.9, 0.8, 0.6, 0.4, 0.2, 0.0]\n",
    "        # ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "992a2756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dummy-HX4ouRvTMavWpkRTwrR5F9\n",
      "Will run validation with these 3D samples (#4): ['1001-mr', '1002-mr', '1003-mr', '1004-mr']\n",
      "Param count model: 556732\n",
      "Generating fresh 'BlendowskiAE' model, optimizer and grad scaler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:07<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 0\n",
      "### Training\n",
      "losses/loss_fold0 0.802249550819397\n",
      "dice_mean_wo_bg_fold0 0.00%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.24336546659469604\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 1\n",
      "### Training\n",
      "losses/loss_fold0 0.2627853453159332\n",
      "dice_mean_wo_bg_fold0 0.00%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.3909141421318054\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 2\n",
      "### Training\n",
      "losses/loss_fold0 0.27491113543510437\n",
      "dice_mean_wo_bg_fold0 0.00%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.39399272203445435\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 3\n",
      "### Training\n",
      "losses/loss_fold0 0.27139341831207275\n",
      "dice_mean_wo_bg_fold0 0.00%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.47654739022254944\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 4\n",
      "### Training\n",
      "losses/loss_fold0 0.23737405240535736\n",
      "dice_mean_wo_bg_fold0 0.22%\n",
      "scores/dice_mean_left_myocardium_fold0 0.94%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.17%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 5.254631996154785\n",
      "val_dice_mean_wo_bg_fold0 0.30%\n",
      "scores/val_dice_mean_left_myocardium_fold0 1.49%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 5\n",
      "### Training\n",
      "losses/loss_fold0 0.4663217067718506\n",
      "dice_mean_wo_bg_fold0 0.81%\n",
      "scores/dice_mean_left_myocardium_fold0 1.88%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 2.15%\n",
      "scores/dice_mean_right_ventricle_fold0 0.03%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.4627766013145447\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 6\n",
      "### Training\n",
      "losses/loss_fold0 0.3186449408531189\n",
      "dice_mean_wo_bg_fold0 0.09%\n",
      "scores/dice_mean_left_myocardium_fold0 0.47%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 0.00%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.5379093885421753\n",
      "val_dice_mean_wo_bg_fold0 0.19%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.42%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 7\n",
      "### Training\n",
      "losses/loss_fold0 0.2638176679611206\n",
      "dice_mean_wo_bg_fold0 1.01%\n",
      "scores/dice_mean_left_myocardium_fold0 0.43%\n",
      "scores/dice_mean_left_atrium_fold0 1.07%\n",
      "scores/dice_mean_left_ventricle_fold0 0.24%\n",
      "scores/dice_mean_right_atrium_fold0 0.00%\n",
      "scores/dice_mean_right_ventricle_fold0 3.29%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 6.588339328765869\n",
      "val_dice_mean_wo_bg_fold0 0.24%\n",
      "scores/val_dice_mean_left_myocardium_fold0 1.22%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 8\n",
      "### Training\n",
      "losses/loss_fold0 0.34405991435050964\n",
      "dice_mean_wo_bg_fold0 0.80%\n",
      "scores/dice_mean_left_myocardium_fold0 0.27%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/dice_mean_right_atrium_fold0 1.87%\n",
      "scores/dice_mean_right_ventricle_fold0 1.84%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.24131126701831818\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 9\n",
      "### Training\n",
      "losses/loss_fold0 0.18396805226802826\n",
      "dice_mean_wo_bg_fold0 0.39%\n",
      "scores/dice_mean_left_myocardium_fold0 0.31%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 0.04%\n",
      "scores/dice_mean_right_atrium_fold0 0.57%\n",
      "scores/dice_mean_right_ventricle_fold0 1.02%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.5875356197357178\n",
      "val_dice_mean_wo_bg_fold0 0.00%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 0.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 10\n",
      "### Training\n",
      "losses/loss_fold0 0.16570119559764862\n",
      "dice_mean_wo_bg_fold0 1.68%\n",
      "scores/dice_mean_left_myocardium_fold0 0.74%\n",
      "scores/dice_mean_left_atrium_fold0 0.00%\n",
      "scores/dice_mean_left_ventricle_fold0 1.01%\n",
      "scores/dice_mean_right_atrium_fold0 2.68%\n",
      "scores/dice_mean_right_ventricle_fold0 3.99%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.31954425573349\n",
      "val_dice_mean_wo_bg_fold0 2.37%\n",
      "scores/val_dice_mean_left_myocardium_fold0 2.44%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.28%\n",
      "scores/val_dice_mean_right_atrium_fold0 3.63%\n",
      "scores/val_dice_mean_right_ventricle_fold0 5.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 11\n",
      "### Training\n",
      "losses/loss_fold0 0.14947910606861115\n",
      "dice_mean_wo_bg_fold0 4.11%\n",
      "scores/dice_mean_left_myocardium_fold0 2.12%\n",
      "scores/dice_mean_left_atrium_fold0 0.05%\n",
      "scores/dice_mean_left_ventricle_fold0 2.75%\n",
      "scores/dice_mean_right_atrium_fold0 7.45%\n",
      "scores/dice_mean_right_ventricle_fold0 8.19%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.24541562795639038\n",
      "val_dice_mean_wo_bg_fold0 3.01%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.00%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.11%\n",
      "scores/val_dice_mean_right_ventricle_fold0 14.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 12\n",
      "### Training\n",
      "losses/loss_fold0 0.13901212811470032\n",
      "dice_mean_wo_bg_fold0 4.15%\n",
      "scores/dice_mean_left_myocardium_fold0 0.01%\n",
      "scores/dice_mean_left_atrium_fold0 0.30%\n",
      "scores/dice_mean_left_ventricle_fold0 1.39%\n",
      "scores/dice_mean_right_atrium_fold0 3.89%\n",
      "scores/dice_mean_right_ventricle_fold0 15.18%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.3988123834133148\n",
      "val_dice_mean_wo_bg_fold0 3.60%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.12%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.35%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.06%\n",
      "scores/val_dice_mean_right_ventricle_fold0 17.47%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 13\n",
      "### Training\n",
      "losses/loss_fold0 0.1260732114315033\n",
      "dice_mean_wo_bg_fold0 4.50%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.33%\n",
      "scores/dice_mean_left_ventricle_fold0 1.22%\n",
      "scores/dice_mean_right_atrium_fold0 2.80%\n",
      "scores/dice_mean_right_ventricle_fold0 18.14%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.23644593358039856\n",
      "val_dice_mean_wo_bg_fold0 4.14%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.12%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.87%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.05%\n",
      "scores/val_dice_mean_right_ventricle_fold0 19.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 14\n",
      "### Training\n",
      "losses/loss_fold0 0.12241567671298981\n",
      "dice_mean_wo_bg_fold0 4.71%\n",
      "scores/dice_mean_left_myocardium_fold0 0.01%\n",
      "scores/dice_mean_left_atrium_fold0 0.08%\n",
      "scores/dice_mean_left_ventricle_fold0 1.39%\n",
      "scores/dice_mean_right_atrium_fold0 3.90%\n",
      "scores/dice_mean_right_ventricle_fold0 18.16%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.29815027117729187\n",
      "val_dice_mean_wo_bg_fold0 4.08%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 0.63%\n",
      "scores/val_dice_mean_right_atrium_fold0 0.03%\n",
      "scores/val_dice_mean_right_ventricle_fold0 19.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 15\n",
      "### Training\n",
      "losses/loss_fold0 0.11475086212158203\n",
      "dice_mean_wo_bg_fold0 7.39%\n",
      "scores/dice_mean_left_myocardium_fold0 0.00%\n",
      "scores/dice_mean_left_atrium_fold0 0.30%\n",
      "scores/dice_mean_left_ventricle_fold0 7.88%\n",
      "scores/dice_mean_right_atrium_fold0 9.57%\n",
      "scores/dice_mean_right_ventricle_fold0 19.21%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.1920798122882843\n",
      "val_dice_mean_wo_bg_fold0 7.09%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.01%\n",
      "scores/val_dice_mean_left_atrium_fold0 1.09%\n",
      "scores/val_dice_mean_left_ventricle_fold0 11.77%\n",
      "scores/val_dice_mean_right_atrium_fold0 1.80%\n",
      "scores/val_dice_mean_right_ventricle_fold0 20.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 16\n",
      "### Training\n",
      "losses/loss_fold0 0.11025016009807587\n",
      "dice_mean_wo_bg_fold0 7.55%\n",
      "scores/dice_mean_left_myocardium_fold0 0.26%\n",
      "scores/dice_mean_left_atrium_fold0 0.62%\n",
      "scores/dice_mean_left_ventricle_fold0 13.63%\n",
      "scores/dice_mean_right_atrium_fold0 8.80%\n",
      "scores/dice_mean_right_ventricle_fold0 14.44%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.1630072146654129\n",
      "val_dice_mean_wo_bg_fold0 11.20%\n",
      "scores/val_dice_mean_left_myocardium_fold0 1.60%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.14%\n",
      "scores/val_dice_mean_left_ventricle_fold0 16.71%\n",
      "scores/val_dice_mean_right_atrium_fold0 18.11%\n",
      "scores/val_dice_mean_right_ventricle_fold0 19.47%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 17\n",
      "### Training\n",
      "losses/loss_fold0 0.10630297660827637\n",
      "dice_mean_wo_bg_fold0 11.49%\n",
      "scores/dice_mean_left_myocardium_fold0 1.99%\n",
      "scores/dice_mean_left_atrium_fold0 0.20%\n",
      "scores/dice_mean_left_ventricle_fold0 18.99%\n",
      "scores/dice_mean_right_atrium_fold0 21.48%\n",
      "scores/dice_mean_right_ventricle_fold0 14.80%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.24807296693325043\n",
      "val_dice_mean_wo_bg_fold0 8.57%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.46%\n",
      "scores/val_dice_mean_left_atrium_fold0 1.12%\n",
      "scores/val_dice_mean_left_ventricle_fold0 19.68%\n",
      "scores/val_dice_mean_right_atrium_fold0 7.83%\n",
      "scores/val_dice_mean_right_ventricle_fold0 13.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 18\n",
      "### Training\n",
      "losses/loss_fold0 0.10464242845773697\n",
      "dice_mean_wo_bg_fold0 11.18%\n",
      "scores/dice_mean_left_myocardium_fold0 1.57%\n",
      "scores/dice_mean_left_atrium_fold0 0.15%\n",
      "scores/dice_mean_left_ventricle_fold0 20.63%\n",
      "scores/dice_mean_right_atrium_fold0 17.68%\n",
      "scores/dice_mean_right_ventricle_fold0 15.89%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.19057047367095947\n",
      "val_dice_mean_wo_bg_fold0 14.30%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.83%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.00%\n",
      "scores/val_dice_mean_left_ventricle_fold0 20.44%\n",
      "scores/val_dice_mean_right_atrium_fold0 25.56%\n",
      "scores/val_dice_mean_right_ventricle_fold0 24.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 19\n",
      "### Training\n",
      "losses/loss_fold0 0.09145279228687286\n",
      "dice_mean_wo_bg_fold0 14.85%\n",
      "scores/dice_mean_left_myocardium_fold0 1.12%\n",
      "scores/dice_mean_left_atrium_fold0 0.03%\n",
      "scores/dice_mean_left_ventricle_fold0 21.86%\n",
      "scores/dice_mean_right_atrium_fold0 23.17%\n",
      "scores/dice_mean_right_ventricle_fold0 28.06%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.15679937601089478\n",
      "val_dice_mean_wo_bg_fold0 18.41%\n",
      "scores/val_dice_mean_left_myocardium_fold0 0.72%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.03%\n",
      "scores/val_dice_mean_left_ventricle_fold0 14.97%\n",
      "scores/val_dice_mean_right_atrium_fold0 39.20%\n",
      "scores/val_dice_mean_right_ventricle_fold0 37.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 20\n",
      "### Training\n",
      "losses/loss_fold0 0.0867309421300888\n",
      "dice_mean_wo_bg_fold0 16.20%\n",
      "scores/dice_mean_left_myocardium_fold0 2.52%\n",
      "scores/dice_mean_left_atrium_fold0 0.02%\n",
      "scores/dice_mean_left_ventricle_fold0 19.14%\n",
      "scores/dice_mean_right_atrium_fold0 28.03%\n",
      "scores/dice_mean_right_ventricle_fold0 31.32%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.12169606983661652\n",
      "val_dice_mean_wo_bg_fold0 20.23%\n",
      "scores/val_dice_mean_left_myocardium_fold0 2.51%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.05%\n",
      "scores/val_dice_mean_left_ventricle_fold0 11.71%\n",
      "scores/val_dice_mean_right_atrium_fold0 46.00%\n",
      "scores/val_dice_mean_right_ventricle_fold0 40.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 21\n",
      "### Training\n",
      "losses/loss_fold0 0.08383575081825256\n",
      "dice_mean_wo_bg_fold0 16.47%\n",
      "scores/dice_mean_left_myocardium_fold0 4.86%\n",
      "scores/dice_mean_left_atrium_fold0 0.16%\n",
      "scores/dice_mean_left_ventricle_fold0 15.15%\n",
      "scores/dice_mean_right_atrium_fold0 33.47%\n",
      "scores/dice_mean_right_ventricle_fold0 28.72%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.19580504298210144\n",
      "val_dice_mean_wo_bg_fold0 18.40%\n",
      "scores/val_dice_mean_left_myocardium_fold0 7.78%\n",
      "scores/val_dice_mean_left_atrium_fold0 0.19%\n",
      "scores/val_dice_mean_left_ventricle_fold0 20.51%\n",
      "scores/val_dice_mean_right_atrium_fold0 34.99%\n",
      "scores/val_dice_mean_right_ventricle_fold0 28.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 22\n",
      "### Training\n",
      "losses/loss_fold0 0.07940647006034851\n",
      "dice_mean_wo_bg_fold0 15.39%\n",
      "scores/dice_mean_left_myocardium_fold0 4.77%\n",
      "scores/dice_mean_left_atrium_fold0 0.89%\n",
      "scores/dice_mean_left_ventricle_fold0 10.16%\n",
      "scores/dice_mean_right_atrium_fold0 32.81%\n",
      "scores/dice_mean_right_ventricle_fold0 28.32%\n",
      "\n",
      "### Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "losses/val_loss_fold0 0.13711872696876526\n",
      "val_dice_mean_wo_bg_fold0 22.91%\n",
      "scores/val_dice_mean_left_myocardium_fold0 12.66%\n",
      "scores/val_dice_mean_left_atrium_fold0 2.44%\n",
      "scores/val_dice_mean_left_ventricle_fold0 26.72%\n",
      "scores/val_dice_mean_right_atrium_fold0 38.16%\n",
      "scores/val_dice_mean_right_ventricle_fold0 34.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch::  25%|██▌       | 1/4 [00:02<00:07,  2.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     wandb\u001b[39m.\u001b[39magent(sweep_id, function\u001b[39m=\u001b[39msweep_run)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     normal_run()\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 15\u001b[0m in \u001b[0;36mnormal_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# training_dataset = prepare_data(config_dict)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m train_DL(run_name, config, training_dataset)\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 15\u001b[0m in \u001b[0;36mtrain_DL\u001b[0;34m(run_name, config, training_dataset)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=183'>184</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=185'>186</a>\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m*\u001b[39mio_normalisation_values[\u001b[39m'\u001b[39;49m\u001b[39mtarget_std\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(b_input\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m+\u001b[39mio_normalisation_values[\u001b[39m'\u001b[39m\u001b[39mtarget_mean\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(b_input\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39m### Calculate loss ###\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def normal_run():\n",
    "    with wandb.init(project=PROJECT_NAME, group=\"training\", job_type=\"train\",\n",
    "            config=config_dict, settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        ) as run:\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        # training_dataset = prepare_data(config_dict)\n",
    "        config = wandb.config\n",
    "\n",
    "        run_dl(run_name, config, training_dataset, test_dataset)\n",
    "\n",
    "def sweep_run():\n",
    "    with wandb.init() as run:\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(start_method=\"thread\"),\n",
    "            mode=config_dict['wandb_mode']\n",
    "        )\n",
    "\n",
    "        run_name = run.name\n",
    "        print(\"Running\", run_name)\n",
    "        # training_dataset = prepare_data(config)\n",
    "        config = wandb.config\n",
    "\n",
    "        run_dl(run_name, config, training_dataset, test_dataset)\n",
    "\n",
    "if config_dict['do_sweep']:\n",
    "    # Integrate all config_dict entries into sweep_dict.parameters -> sweep overrides config_dict\n",
    "    cp_config_dict = copy.deepcopy(config_dict)\n",
    "    # cp_config_dict.update(copy.deepcopy(sweep_config_dict['parameters']))\n",
    "    for del_key in sweep_config_dict['parameters'].keys():\n",
    "        if del_key in cp_config_dict:\n",
    "            del cp_config_dict[del_key]\n",
    "    merged_sweep_config_dict = copy.deepcopy(sweep_config_dict)\n",
    "    # merged_sweep_config_dict.update(cp_config_dict)\n",
    "    for key, value in cp_config_dict.items():\n",
    "        merged_sweep_config_dict['parameters'][key] = dict(value=value)\n",
    "    # Convert enum values in parameters to string. They will be identified by their numerical index otherwise\n",
    "    for key, param_dict in merged_sweep_config_dict['parameters'].items():\n",
    "        if 'value' in param_dict and isinstance(param_dict['value'], Enum):\n",
    "            param_dict['value'] = str(param_dict['value'])\n",
    "        if 'values' in param_dict:\n",
    "            param_dict['values'] = [str(elem) if isinstance(elem, Enum) else elem for elem in param_dict['values']]\n",
    "\n",
    "        merged_sweep_config_dict['parameters'][key] = param_dict\n",
    "\n",
    "    sweep_id = wandb.sweep(merged_sweep_config_dict, project=PROJECT_NAME)\n",
    "    wandb.agent(sweep_id, function=sweep_run)\n",
    "\n",
    "else:\n",
    "    normal_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90b54c12",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_notebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_notebook():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     sys\u001b[39m.\u001b[39mexit(\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'in_notebook' is not defined"
     ]
    }
   ],
   "source": [
    "if not in_notebook():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af1cf84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dummy-EzxVz3eRm8H7haihXtEusU\n",
      "Will run validation with these 3D samples (#4): ['1001-mr', '1002-mr', '1003-mr', '1004-mr']\n",
      "Param count model: 556732\n",
      "Generating fresh 'BlendowskiAE' model, optimizer and grad scaler.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch:: 100%|██████████| 4/4 [00:11<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Log epoch 0\n",
      "### Training\n",
      "losses/loss_fold0 3.6996378898620605\n",
      "losses/loss_fold0 3.6996378898620605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReduceLROnPlateau' object has no attribute 'get_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 364>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=384'>385</a>\u001b[0m     wandb\u001b[39m.\u001b[39magent(sweep_id, function\u001b[39m=\u001b[39msweep_run)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=386'>387</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=387'>388</a>\u001b[0m     normal_run()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=389'>390</a>\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=390'>391</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m in_notebook():\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36mnormal_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=344'>345</a>\u001b[0m \u001b[39m# training_dataset = prepare_data(config_dict)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=345'>346</a>\u001b[0m config \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=347'>348</a>\u001b[0m train_DL(run_name, config, training_dataset)\n",
      "\u001b[1;32m/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb Cell 11\u001b[0m in \u001b[0;36mtrain_DL\u001b[0;34m(run_name, config, training_dataset)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=205'>206</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlosses/loss_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=206'>207</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlosses/loss_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mscheduler_lr\u001b[39m\u001b[39m\"\u001b[39m, scheduler\u001b[39m.\u001b[39;49mget_lr())\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=209'>210</a>\u001b[0m mean_dice \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnanmean(dices)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Brechenknecht01/share/data_supergrover1/weihsbach/shared_data/tmp/slice_inflate/main_slice_inflate.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdice_mean_wo_bg_fold\u001b[39m\u001b[39m{\u001b[39;00mfold_idx\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmean_dice\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReduceLROnPlateau' object has no attribute 'get_lr'"
     ]
    }
   ],
   "source": [
    "# Do any postprocessing / visualization in notebook here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d208705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "50e8264c9cb9bb8a6cada87af39a6b7aa8c2638398580ca823279198d429a8d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
